# filename: newp/browser_use/agent/service.py
# ... (other imports remain the same) ...
import asyncio
import gc
import inspect
import json
import logging
import os
import re 
import time
import traceback
import uuid
from pathlib import Path
from typing import Any, Type, Awaitable, Callable, Dict, Generic, List, Optional, Tuple, TypeVar, Union
from datetime import datetime
import hashlib
from browser_use.utils import redact_sensitive_data

from dotenv import load_dotenv
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from pydantic import ValidationError

from browser_use.agent.gif import create_history_gif
from browser_use.agent.message_manager.service import MessageManager, MessageManagerSettings
from browser_use.dom.history_tree_processor.view import DOMHistoryElement
from browser_use.agent.message_manager.utils import convert_input_messages, extract_json_from_model_output, save_conversation_step_to_file
from browser_use.agent.prompts import AgentMessagePrompt, PlannerPrompt, SystemPrompt
from browser_use.agent.views import (
    ActionResult,
    AgentBrain,
    AgentHistory,
    AgentHistoryList,
    AgentOutput,
    AgentSettings,
    AgentState,
    AgentStatus,
    AgentStepInfo,
    ReflectionPlannerOutput,
    StepMetadata,
    ToolCallingMethod,
)
# ActionModel base for dynamic model creation comes from controller.registry
from browser_use.controller.registry.views import ActionModel

from browser_use.browser.browser import Browser
from browser_use.browser.context import BrowserContext
from browser_use.browser.views import BrowserState, BrowserStateHistory # BrowserState for live, BrowserStateHistory for records

from browser_use.controller.service import Controller
from browser_use.exceptions import (
    AgentException, 
    AgentConfigurationError, 
    LLMException, 
    LLMCommunicationError, 
    LLMOutputParsingError,
    AgentInterruptedError # Added for explicit handling in step
)
from browser_use.telemetry.service import ProductTelemetry
from browser_use.telemetry.views import (
    AgentEndTelemetryEvent,
    AgentRunTelemetryEvent,
    AgentStepTelemetryEvent,
)
from browser_use.utils import check_env_variables, time_execution_async, time_execution_sync, SignalHandler, Timeout 

from browser_use.local_pc_tools import (
    write_agent_text_file as pc_tool_write_agent_text_file, 
    ToolError as LocalPCToolError, 
    AGENT_DIR_PATH 
)


load_dotenv()
logger = logging.getLogger(__name__)

SKIP_LLM_API_KEY_VERIFICATION = os.environ.get('SKIP_LLM_API_KEY_VERIFICATION', 'false').lower() in ['true', 't', '1', 'yes', 'y']

def agent_log(level: int, agent_id: Optional[str], step: Optional[int], message: str, **kwargs):
    """Helper for structured logging within Agent methods."""
    log_extras = {'agent_id': agent_id or 'N/A', 'step': step if step is not None else 'N/A'}
    exc_info_val = kwargs.pop('exc_info', None) 
    log_extras.update(kwargs) 
    logger.log(level, message, exc_info=exc_info_val, extra=log_extras)


def log_agent_output_structured(response: AgentOutput, agent_id: str, step: int, agent_instance: Optional['Agent'] = None) -> None:
    """Logs the structured AgentOutput (AgentBrain and Actions) using agent_log."""
    assessment = response.current_state.prior_action_assessment
    emoji = 'ðŸ¤”' 
    if 'success' in assessment.lower(): emoji = 'âœ…'
    elif 'fail' in assessment.lower() or 'error' in assessment.lower(): emoji = 'âŒ'

    agent_log(logging.INFO, agent_id, step, f'{emoji} Assessment: {assessment}')
    agent_log(logging.INFO, agent_id, step, f'ðŸ“Š Task Log: {response.current_state.task_log}')
    agent_log(logging.INFO, agent_id, step, f'ðŸŽ¯ Next Goal (LLM): {response.current_state.next_goal}')

    sensitive_data_map = None
    if agent_instance and hasattr(agent_instance, 'sensitive_data'):
        sensitive_data_map = agent_instance.sensitive_data

    if response.action: 
        for i, action_model_instance in enumerate(response.action):
            action_dump = action_model_instance.model_dump(exclude_unset=True)
            action_name = next(iter(action_dump.keys())) if action_dump else "unknown_action"
            action_params = action_dump.get(action_name, {})

            params_json_str = json.dumps(action_params, ensure_ascii=False)
            params_str_for_log = redact_sensitive_data(params_json_str, sensitive_data_map)

            if len(params_str_for_log) > 150 and params_str_for_log != "[SENSITIVE_DATA]": 
                params_str_for_log = params_str_for_log[:147] + "..."

            agent_log(logging.INFO, agent_id, step, f'ðŸ› ï¸ Action {i + 1}/{len(response.action)}: {action_name}({params_str_for_log})')
    else:
        agent_log(logging.INFO, agent_id, step, 'ðŸ› ï¸ No actions planned by LLM.')


ContextT = TypeVar('ContextT') 
AgentHookFunc = Callable[['Agent'], Awaitable[None]]

MAX_LLM_RETRIES = int(os.getenv("MAX_LLM_RETRIES", "3")) 
HISTORY_FOR_PLANNER = int(os.getenv("HISTORY_FOR_PLANNER", "5")) 

REQUIRED_LLM_API_ENV_VARS_AGENT_SERVICE = {
	'ChatOpenAI': ['OPENAI_API_KEY'],
	'AzureChatOpenAI': ['AZURE_OPENAI_API_KEY', 'AZURE_OPENAI_ENDPOINT'], 
    'ChatGoogleGenerativeAI': ['GOOGLE_API_KEY'], 
    'RotatingGeminiClient': ['GOOGLE_API_KEY'], 
}


class Agent(Generic[ContextT]):
    state: AgentState
    settings: AgentSettings
    llm: BaseChatModel
    planner_llm: Optional[BaseChatModel]
    controller: Controller[ContextT]
    browser: Browser
    browser_context: BrowserContext
    _message_manager: MessageManager
    signal_handler: SignalHandler
    context: Optional[ContextT]
    
    version: str = "unknown"
    source: str = "unknown"
    model_name: str = "Unknown"
    planner_model_name: Optional[str] = None
    chat_model_library: str = "Unknown"

    ActionModel: Type[ActionModel] = ActionModel 
    AgentOutput: Type[AgentOutput] = AgentOutput
    DoneActionModel: Type[ActionModel] = ActionModel
    DoneAgentOutput: Type[AgentOutput] = AgentOutput

    initial_actions_parsed: Optional[List[ActionModel]] = None
    unfiltered_actions_description: str = ""
    
    telemetry: ProductTelemetry # Added to match user-provided service.py

    register_new_step_callback: Optional[Callable[[AgentHistory], Awaitable[None]]] = None
    register_done_callback: Optional[Callable[[AgentHistoryList], Awaitable[None]]] = None
    
    _last_prompt_tokens_for_step: Optional[int] = None
    _last_completion_tokens_for_step: Optional[int] = None
    _last_total_tokens_for_step: Optional[int] = None

    def __init__(
        self,
        task: Union[str, Dict[str, Any]],
        llm: BaseChatModel, 
        settings: Optional[AgentSettings] = None,
        browser_instance: Optional[Browser] = None,
        browser_context_instance: Optional[BrowserContext] = None,
        controller_instance: Optional[Controller[ContextT]] = None,
        sensitive_data_map: Optional[Dict[str, str]] = None,
        initial_actions_raw: Optional[List[Dict[str, Dict[str, Any]]]] = None,
        register_new_step_callback: Optional[Callable[[AgentHistory], Awaitable[None]]] = None,
        register_done_callback: Optional[Callable[[AgentHistoryList], Awaitable[None]]] = None,
        injected_agent_state: Optional[AgentState] = None,
        custom_context: Optional[ContextT] = None,
        **kwargs 
    ):
        self.state = injected_agent_state or AgentState()
        if not self.state.agent_id: 
            self.state.agent_id = str(uuid.uuid4())
        initial_log_id = self.state.agent_id

        if settings: 
            if not isinstance(settings, AgentSettings):
                agent_log(logging.WARNING, initial_log_id, 0, f"Parameter 'settings' is not an AgentSettings instance (type: {type(settings)}). Will use its dict representation if possible, or AgentSettings defaults.")
                if isinstance(settings, dict):
                    self.settings = AgentSettings(**settings) 
                else: 
                    self.settings = AgentSettings(**kwargs) 
            else: 
                 self.settings = settings
                 if kwargs: 
                    valid_setting_kwargs = {k: v for k, v in kwargs.items() if k in AgentSettings.model_fields}
                    if valid_setting_kwargs:
                        self.settings = self.settings.model_copy(update=valid_setting_kwargs)
        else: 
            self.settings = AgentSettings(**kwargs) 

        agent_log(logging.DEBUG, initial_log_id, 0, f"Final AgentSettings.use_planner: {self.settings.use_planner}")
        agent_log(logging.DEBUG, initial_log_id, 0, f"Final AgentSettings.planner_llm is None: {self.settings.planner_llm is None}")
        agent_log(logging.DEBUG, initial_log_id, 0, f"Final AgentSettings.planner_llm type: {type(self.settings.planner_llm)}")

        if not isinstance(llm, BaseChatModel):
            err_msg = f"Provided 'llm' is not a valid BaseChatModel. Got: {type(llm)}. Agent cannot operate."
            agent_log(logging.CRITICAL, initial_log_id, 0, err_msg)
            raise AgentConfigurationError(err_msg)
        self.llm = llm 
        agent_log(logging.DEBUG, initial_log_id, 0, f"Main LLM (self.llm) type: {type(self.llm)}, is None: {self.llm is None}")

        self.controller = controller_instance or Controller[ContextT]()
        self.sensitive_data = sensitive_data_map
        self.context = custom_context
        self.telemetry = ProductTelemetry() # Initialize telemetry

        self.planner_llm = None 

        if self.settings.use_planner:
            agent_log(logging.DEBUG, initial_log_id, 0, "use_planner is True. Evaluating settings.planner_llm.")
            if self.settings.planner_llm is None: 
                agent_log(logging.INFO, initial_log_id, 0, 
                          "Agent settings: use_planner=True and settings.planner_llm is None. Defaulting Agent's planner_llm to main agent LLM.")
                if self.llm is not None and isinstance(self.llm, BaseChatModel):
                    self.planner_llm = self.llm
                    agent_log(logging.INFO, initial_log_id, 0, "Successfully defaulted Agent.planner_llm to main LLM.")
                else:
                    agent_log(logging.ERROR, initial_log_id, 0, "Cannot default planner_llm: Main LLM (self.llm) is None or invalid. Planner will be effectively disabled.")
                    self.planner_llm = None 
            elif not isinstance(self.settings.planner_llm, BaseChatModel):
                 err_msg = f"Provided settings.planner_llm is not a valid BaseChatModel. Got: {type(self.settings.planner_llm)}. Planner will be disabled."
                 agent_log(logging.ERROR, initial_log_id, 0, err_msg)
                 self.planner_llm = None 
            else: 
                self.planner_llm = self.settings.planner_llm
                agent_log(logging.INFO, initial_log_id, 0, 
                          f"Using explicitly provided settings.planner_llm for Agent.planner_llm. Type: {type(self.planner_llm)}")
        else: 
            self.planner_llm = None 
            agent_log(logging.INFO, initial_log_id, 0, "Agent settings: use_planner is False. Planner will not be used.")
        
        agent_log(logging.DEBUG, initial_log_id, 0, f"After defaulting logic: Agent.planner_llm is None: {self.planner_llm is None}, Type: {type(self.planner_llm)}")
        
        if isinstance(task, str):
            self.state.task = {"id": self.state.agent_id, "instructions": task, "user_id": "default_user"}
        elif isinstance(task, dict):
            self.state.task = {"id": task.get('id', self.state.agent_id), "instructions": task.get('instructions', 'No instructions provided.'), "user_id": task.get('user_id', 'default_user'), **task }
        else: 
            raise AgentConfigurationError("Task must be a string (instructions) or a dictionary.")
        
        self.state.max_steps = self.settings.max_steps

        self._set_model_names() 
        self._setup_action_models()
        self._set_browser_use_version_and_source()
        if initial_actions_raw:
            try: self.initial_actions_parsed = self._convert_initial_actions_to_models(initial_actions_raw)
            except Exception as e: raise AgentConfigurationError(f"Error parsing initial_actions: {e}") from e
        
        self.tool_calling_method = None 
        self._set_tool_calling_method()
        self._handle_deepseek_vision_setting()

        planner_llm_log_status = "Not Used (use_planner is False or planner_llm is None)"
        if self.settings.use_planner:
            if self.planner_llm: 
                if self.planner_llm == self.llm: planner_llm_log_status = f"Defaulted to Main LLM ({self.planner_model_name or 'Name N/A'})"
                else: planner_llm_log_status = self.planner_model_name or "Custom Planner LLM (Name N/A)"
            else: planner_llm_log_status = "WARNING: use_planner=True but planner_llm is still None!"
        
        agent_log(logging.INFO, self.state.agent_id, 0,
            f'Agent "{self.state.agent_id}" initialized. Task: "{self.state.task.get("instructions", "N/A")[:50]}..."'
            f'\nMain LLM: {self.model_name or "N/A"} ({self.chat_model_library or "N/A"}{self._get_tool_log_str()}){" +Vision" if self.settings.use_vision else ""}.'
            f'\nPlanner LLM: {planner_llm_log_status}{(" +Vision" if self.settings.use_vision_for_planner and self.planner_llm else "")}.'
            f'\nReflection Cycle: {"Enabled" if self.settings.use_planner and self.planner_llm else "Disabled"} (Interval: {self.settings.planner_interval} steps).'
            f'\nMax Steps: {self.state.max_steps}, Max Actions/Step: {self.settings.max_actions_per_step}, Max Failures: {self.settings.max_failures}.'
        )
        
        self.unfiltered_actions_description = self.controller.registry.get_prompt_description()
        system_prompt_obj = SystemPrompt(
            max_actions_per_step=self.settings.max_actions_per_step,
            override_system_message=self.settings.override_system_message,
            extend_system_message=self.settings.extend_system_message,
        )
        self._message_manager = MessageManager(
            task_description=self.state.task.get('instructions', 'Undefined task'),
            system_prompt_message=system_prompt_obj.get_system_message(),
            settings=MessageManagerSettings(
                max_input_tokens=self.settings.max_input_tokens, include_attributes=self.settings.include_attributes,
                message_context=self.settings.message_context, sensitive_data=self.sensitive_data,
                available_file_paths=self.settings.available_file_paths, image_tokens=self.settings.image_tokens,
                estimated_characters_per_token=self.settings.estimated_characters_per_token,
                recent_message_window_priority=self.settings.recent_message_window_priority
            ), state=self.state.message_manager_state )

        self.browser = browser_instance or Browser(config=self.settings.browser_config)
        self.injected_browser = bool(browser_instance)

        self.browser_context = browser_context_instance or BrowserContext(browser=self.browser, config=self.settings.default_browser_context_config)
        self.injected_browser_context = bool(browser_context_instance)
        self.register_new_step_callback = register_new_step_callback
        self.register_done_callback = register_done_callback
        
        try: loop = asyncio.get_event_loop_policy().get_event_loop()
        except RuntimeError: loop = asyncio.new_event_loop(); asyncio.set_event_loop(loop)
        if loop.is_closed(): loop = asyncio.new_event_loop(); asyncio.set_event_loop(loop)
        self.signal_handler = SignalHandler(loop=loop, pause_callback=self.pause, resume_callback=self.resume, custom_exit_callback=self._on_exit_signal)

        if self.settings.save_conversation_path:
            self.settings.save_conversation_path = str(Path(self.settings.save_conversation_path).resolve())
            agent_log(logging.INFO, self.state.agent_id, 0, f'Conversation steps will be saved to: {self.settings.save_conversation_path}')
    
    def _get_tool_log_str(self) -> str:
         method = self.settings.tool_calling_method
         if method == 'function_calling': return " +Tools(FuncCall)"
         if method == 'raw': return " +Tools(RawParse)" 
         if method == 'json_mode': return " +Tools(JSONMode)" 
         return "" 

    def _handle_deepseek_vision_setting(self):
        if self.model_name and 'deepseek' in self.model_name.lower() and self.settings.use_vision:
            agent_log(logging.WARNING, self.state.agent_id, 0, 'DeepSeek models via some Langchain integrations might have issues with direct vision input. Verifying settings. Consider disabling vision if issues occur.')
        if self.planner_model_name and 'deepseek' in self.planner_model_name.lower() and self.settings.use_vision_for_planner:
            agent_log(logging.WARNING, self.state.agent_id, 0, 'DeepSeek planner LLM vision setting active. Verify compatibility.')
            
    def _set_message_context_for_raw_tool_calling(self) -> Optional[str]:
        if self.settings.tool_calling_method == 'raw':
            pass
        return self.settings.message_context 
        
    def _set_browser_use_version_and_source(self) -> None:
        try:
            import importlib.metadata
            try:
                self.version = importlib.metadata.version('browser-use') 
                self.source = 'pip-installed'
            except importlib.metadata.PackageNotFoundError:
                self.version = 'unknown-dev'
                self.source = 'local-dev'
        except Exception:
            self.version = 'unknown'
            self.source = 'unknown'
        agent_log(logging.DEBUG, self.state.agent_id, 0, f'Agent Version: {self.version}, Source: {self.source}')

    def _set_model_names(self) -> None:
        if self.llm:
            self.chat_model_library = self.llm.__class__.__name__
            self.model_name = getattr(self.llm, 'model_name', getattr(self.llm, 'model', 'UnknownLLM'))
        if self.settings.planner_llm: # Changed from self.planner_llm to self.settings.planner_llm as self.planner_llm might not be set yet
            self.planner_model_name = getattr(self.settings.planner_llm, 'model_name', getattr(self.settings.planner_llm, 'model', 'UnknownPlannerLLM'))
        else:
            self.planner_model_name = None


    def _setup_action_models(self) -> None:
        self.ActionModel = self.controller.registry.create_action_model(page=None) 
        self.AgentOutput = AgentOutput.type_with_custom_actions(self.ActionModel)
        
        self.DoneActionModel = self.controller.registry.create_action_model(include_actions=['done'])
        self.DoneAgentOutput = AgentOutput.type_with_custom_actions(self.DoneActionModel)

    def _convert_initial_actions_to_models(self, raw_actions_list: List[Dict[str, Dict[str, Any]]]) -> List[ActionModel]:
        parsed_actions: List[ActionModel] = []
        current_action_schema = self.ActionModel 

        for action_dict_outer in raw_actions_list:
            if not isinstance(action_dict_outer, dict) or len(action_dict_outer) != 1:
                raise AgentConfigurationError(f"Invalid initial action format: {action_dict_outer}. Expected single key-value pair like {{'action_name': {{params...}}}}.")
            
            action_name = next(iter(action_dict_outer))
            action_params_dict = action_dict_outer[action_name]
            if action_params_dict is None: action_params_dict = {} 

            try:
                action_instance = current_action_schema.model_validate(action_dict_outer)
                parsed_actions.append(action_instance)
            except ValidationError as e: 
                raise AgentConfigurationError(f"Validation error for initial action '{action_name}' with params {action_params_dict}: {e}")
            except Exception as e: 
                raise AgentConfigurationError(f"Error converting initial action '{action_name}': {e}")
        return parsed_actions
        
    def _set_tool_calling_method(self) -> None:
        current_method = self.settings.tool_calling_method
        if current_method == 'auto':
            model_name_lower = self.model_name.lower() if self.model_name else ""
            chat_library_name = self.chat_model_library or ""

            if 'deepseek-reasoner' in model_name_lower or 'deepseek-r1' in model_name_lower:
                self.settings.tool_calling_method = 'raw' 
            elif chat_library_name == 'ChatGoogleGenerativeAI' or chat_library_name == 'RotatingGeminiClient':
                self.settings.tool_calling_method = 'function_calling' 
            elif chat_library_name in ['ChatOpenAI', 'AzureChatOpenAI']:
                self.settings.tool_calling_method = 'function_calling'
            else:
                logger.info(f"Tool calling method 'auto' for {chat_library_name} / {model_name_lower}: defaulting to None (manual JSON expected via prompt). Consider 'json_mode' or 'function_calling' if supported.")
                self.settings.tool_calling_method = None 
            agent_log(logging.DEBUG, self.state.agent_id, 0, f"Auto-detected tool_calling_method: {self.settings.tool_calling_method} for {self.model_name}")
        
    async def _verify_llm_connection(self, llm_instance: BaseChatModel, llm_name_for_log: str) -> bool:
        if not llm_instance: 
            return False
            
        if getattr(llm_instance, '_verified_api_keys', False) and not os.getenv("FORCE_LLM_VERIFICATION"):
            return True

        llm_class_name = llm_instance.__class__.__name__
        required_keys = REQUIRED_LLM_API_ENV_VARS_AGENT_SERVICE.get(llm_class_name, [])
        
        if required_keys and not check_env_variables(required_keys, check_all=True): 
            error_msg = (f"{llm_name_for_log} ({llm_class_name}) might be missing required API environment variables: {required_keys}. "
                         f"Set them or set SKIP_LLM_API_KEY_VERIFICATION=true to bypass.")
            agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps, error_msg)
            if not SKIP_LLM_API_KEY_VERIFICATION:
                setattr(llm_instance, '_verified_api_keys', False)
                return False

        if SKIP_LLM_API_KEY_VERIFICATION:
            agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, f"Skipping API key/connection verification for {llm_name_for_log}.")
            setattr(llm_instance, '_verified_api_keys', True)
            return True

        test_prompt = [HumanMessage(content='Briefly explain what an LLM is in one short sentence.')]
        expected_keywords = ['model', 'language', 'text'] 
        try:
            agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, f"Attempting test call for {llm_name_for_log}...")
            
            response = await llm_instance.ainvoke(test_prompt) 
            response_text = str(response.content).lower()

            if any(keyword in response_text for keyword in expected_keywords):
                agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, f"{llm_name_for_log} ({llm_class_name}) connection verified successfully.")
                setattr(llm_instance, '_verified_api_keys', True)
                return True
            else:
                agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps,
                          f"{llm_name_for_log} connection test: Unexpected response. Prompt='{test_prompt[0].content}', Got='{response_text[:100]}...'")
                setattr(llm_instance, '_verified_api_keys', False) 
                return True 
        except Exception as e:
            setattr(llm_instance, '_verified_api_keys', False)
            agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps,
                      f"{llm_name_for_log} ({llm_class_name}) connection test failed: {type(e).__name__} - {e}. Required keys: {required_keys}", 
                      exc_info=logger.isEnabledFor(logging.DEBUG))
            return False

    async def _execute_initial_actions(self) -> None:
        if not self.initial_actions_parsed:
            return

        agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, f"Executing {len(self.initial_actions_parsed)} initial action(s)...")
        
        action_results: List[ActionResult] = await self.multi_act( # Changed from self.controller.multi_act
            actions_from_llm_model_instances=self.initial_actions_parsed, 
            browser_context=self.browser_context,
            page_extraction_llm=self.settings.page_extraction_llm,
            sensitive_data=self.sensitive_data,
            available_file_paths=self.settings.available_file_paths,
            context=self.context
        )
        self.state.last_result = action_results

        try:
            current_browser_state_obj = await self.browser_context.get_state(cache_clickable_elements_hashes=False)
            browser_state_for_history = BrowserStateHistory(
                url=current_browser_state_obj.url,
                title=current_browser_state_obj.title,
                tabs=current_browser_state_obj.tabs,
                interacted_element=[], 
                screenshot=current_browser_state_obj.screenshot
            )
        except Exception as e:
            agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps, f"Failed to get browser state after initial actions: {e}")
            browser_state_for_history = None

        history_item = AgentHistory(
            step=self.state.n_steps, 
            browser_state=browser_state_for_history,
            agent_output=None, 
            action_results=action_results,
            metadata=StepMetadata(
                step_number=self.state.n_steps,
                step_start_time=time.monotonic(), 
                step_end_time=time.monotonic(),
            )
        )
        self.state.history.history.append(history_item)
        
        if any(not ar.success for ar in action_results):
            first_error = next((ar.error for ar in action_results if ar.error), "An initial action failed without a specific error message.")
            self.state.last_error = first_error
            self.state.consecutive_failures += 1 
            agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, f"Initial action(s) resulted in failure: {first_error}. Agent may start in RECOVERING state.")
            
    def _get_browser_state_fingerprint(self, browser_state: BrowserState) -> str:
        if not browser_state:
            agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, "Cannot generate fingerprint for None browser_state.")
            return "" 
        
        url = browser_state.url or "unknown_url"
        title = browser_state.title or "unknown_title"
        
        elements_string = ""
        if browser_state.element_tree and hasattr(browser_state.element_tree, 'clickable_elements_to_string'):
            try:
                fingerprint_attributes = ['id', 'name', 'role', 'aria-label', 'placeholder', 'type', 'href']
                elements_string = browser_state.element_tree.clickable_elements_to_string(include_attributes=fingerprint_attributes)
            except Exception as e:
                agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, f"Error generating elements string for state fingerprint: {e}")
                if browser_state.element_tree and browser_state.element_tree.children:
                    elements_string = f"elements_count:{len(browser_state.element_tree.children)}"

        tab_count = len(browser_state.tabs)
        tab_titles_sorted = ",".join(sorted([t.title for t in browser_state.tabs])) if browser_state.tabs else ""
        
        fingerprint_parts = [
            f"URL:{url}",
            f"TITLE:{title}",
            f"TAB_COUNT:{tab_count}",
            f"TAB_TITLES:{tab_titles_sorted}",
            f"ELEMENTS_STRUCTURE:{elements_string}" 
        ]
        
        fingerprint_string = "||".join(fingerprint_parts)
        return fingerprint_string

    @time_execution_async('--run (agent)')
    async def run(
        self, 
        max_steps: Optional[int] = None,
        on_step_start: Optional[AgentHookFunc] = None, 
        on_step_end: Optional[AgentHookFunc] = None    
    ) -> AgentHistoryList:
        if max_steps is not None: 
            self.state.max_steps = max_steps
        elif self.settings.max_steps is not None: 
            self.state.max_steps = self.settings.max_steps

        if not hasattr(self, 'signal_handler') or self.signal_handler is None:
            self.signal_handler = SignalHandler(
                loop=asyncio.get_event_loop(),
                pause_callback=self.pause,
                resume_callback=self.resume,
                custom_exit_callback=self._on_exit_signal
            )
        self.signal_handler.register()

        llm_verified_flag_attr = '_verified_api_keys'

        if not getattr(self.llm, llm_verified_flag_attr, False): 
            if not await self._verify_llm_connection(self.llm, 'Main LLM'):
                error_msg = "Main LLM connection verification failed. Agent cannot run."
                agent_log(logging.CRITICAL, self.state.agent_id, self.state.n_steps, error_msg)
                self.state.status = AgentStatus.FAILED
                self.state.accumulated_output = error_msg
                self.state.last_error = error_msg
                await self._record_error_in_history(AgentConfigurationError(error_msg)) 
                await self._log_final_status_and_telemetry() 
                return self.state.history

        if self.settings.use_planner and self.settings.planner_llm: # Corrected from self.settings.planner_llm to self.planner_llm which is set in __init__
            if not getattr(self.planner_llm, llm_verified_flag_attr, False): 
                if not await self._verify_llm_connection(self.planner_llm, 'Planner LLM'):
                    agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, 
                              "Planner LLM connection verification failed. Planning/reflection might be impaired or disabled.")

        self._log_agent_run_start_telemetry()


        self.state.status = AgentStatus.PENDING 
        
        try:
            if self.settings.load_initial_summary_from_db:
                 pass

            if self.initial_actions_parsed and self.state.n_steps == 0 and not self.state.history.history:
                 await self._execute_initial_actions()
                 
            self.state.status = AgentStatus.RUNNING 

            while self.state.n_steps < self.state.max_steps:
                current_step_info = AgentStepInfo(step_number=self.state.n_steps, max_steps=self.state.max_steps)

                if self.state.status == AgentStatus.STOPPED:
                    agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "Agent run loop terminated: STOPPED.")
                    break
                if self.state.status == AgentStatus.PAUSED:
                    agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "Agent PAUSED in run loop. Waiting for resume...")
                    while self.state.status == AgentStatus.PAUSED:
                        await asyncio.sleep(0.2) 
                    if self.state.status == AgentStatus.RUNNING:
                        agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "Agent RESUMED in run loop.")
                        if hasattr(self.signal_handler, '_reset_ctrl_c_count'): self.signal_handler._reset_ctrl_c_count()
                    elif self.state.status == AgentStatus.STOPPED:
                        agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "Agent STOPPED during pause in run loop.")
                        break
                    else: 
                        agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, f"Agent exited pause to unexpected status: {self.state.status}. Continuing run loop check.")
                        continue 

                if self.state.consecutive_failures >= self.settings.max_failures:
                    err_msg = f'Max failures ({self.settings.max_failures}) reached. Stopping agent run.'
                    agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps, err_msg)
                    self.state.status = AgentStatus.FAILED
                    self.state.accumulated_output = self.state.last_error or err_msg
                    await self._record_error_in_history(RuntimeError(err_msg)) 
                    break
                
                if on_step_start: 
                    await self._try_run_callback_simple(on_step_start, self)

                task_done_successfully_this_step, step_executed_validly = await self.step(current_step_info)
                
                is_step_valid_overall = self.state.status not in [AgentStatus.FAILED, AgentStatus.STOPPED]

                if on_step_end: 
                    await self._try_run_callback_simple(on_step_end, self)

                if task_done_successfully_this_step: 
                    agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, 
                              f"Agent.run: Task completion signaled as successful by step {self.state.n_steps}.")
                    
                    if self.settings.validate_output: 
                         if not await self._validate_output(): 
                            if self.state.status == AgentStatus.RECOVERING:
                                agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "Output validation failed, agent will attempt recovery in next step.")
                                continue 
                    
                    if hasattr(self, 'log_completion') and callable(getattr(self, 'log_completion')): 
                        await self.log_completion() 
                    break 
                
                if self.state.status == AgentStatus.COMPLETED and not task_done_successfully_this_step:
                    agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps,
                              f"Agent.run: Task marked COMPLETED by step {self.state.n_steps}, but step did not report it as successful. Output: {self.state.accumulated_output}")
                    if hasattr(self, 'log_completion') and callable(getattr(self, 'log_completion')): await self.log_completion()
                    break 

                if not is_step_valid_overall: 
                    agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps,
                              f"Agent.run: Step execution was not valid. Agent status: {self.state.status}. Halting run.")
                    break 

                if self.state.n_steps >= self.state.max_steps: 
                    agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps,
                              f"Agent reached max_steps ({self.state.max_steps}) without task completion via 'done' action.")
                    self.state.status = AgentStatus.MAX_STEPS_REACHED
                    if not self.state.accumulated_output: 
                        last_hist_item = self.state.history.history[-1] if self.state.history.history else None
                        last_assessment = "N/A"
                        if last_hist_item and last_hist_item.agent_output and last_hist_item.agent_output.current_state:
                            last_assessment = last_hist_item.agent_output.current_state.prior_action_assessment
                        self.state.accumulated_output = f"Max steps ({self.state.max_steps}) reached. Last assessment: {last_assessment}"
                    break
                
                if self.settings.step_interval > 0:
                    await asyncio.sleep(self.settings.step_interval)
            
            if self.state.status == AgentStatus.RUNNING: 
                if self.state.n_steps >= self.state.max_steps:
                    agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, "Run loop finished due to max_steps (final check).")
                    self.state.status = AgentStatus.MAX_STEPS_REACHED
                else:
                    agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, 
                              f"Agent run loop finished unexpectedly while status was {self.state.status.value}. Setting to STOPPED.")
                    self.state.status = AgentStatus.STOPPED
                    self.state.accumulated_output = self.state.accumulated_output or "Agent run ended."


        except KeyboardInterrupt: 
            agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "KeyboardInterrupt caught in Agent.run. Agent stopping.")
            self.state.status = AgentStatus.STOPPED 
            self.state.accumulated_output = self.state.accumulated_output or "Agent run interrupted by KeyboardInterrupt."
        except (InterruptedError, asyncio.CancelledError) as e:
             agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, f'Agent run interrupted/cancelled: {type(e).__name__} - {e}. Current status: {self.state.status}')
             if self.state.status not in [AgentStatus.PAUSED, AgentStatus.STOPPED, AgentStatus.FAILED, AgentStatus.COMPLETED]:
                 self.state.status = AgentStatus.STOPPED 
                 self.state.accumulated_output = self.state.accumulated_output or f"Agent run interrupted: {e}"
        except AgentException as e: 
            agent_log(logging.CRITICAL, self.state.agent_id, self.state.n_steps, f"AgentException in run: {e}", exc_info=True)
            self.state.status = AgentStatus.FAILED
            self.state.accumulated_output = f"Agent failed due to AgentException: {e}"
            await self._record_error_in_history(e)
        except Exception as e: 
             agent_log(logging.CRITICAL, self.state.agent_id, self.state.n_steps, f"Unhandled exception in Agent.run: {e}", exc_info=True)
             self.state.status = AgentStatus.FAILED
             self.state.accumulated_output = f"Agent failed with unhandled exception: {e}"
             await self._record_error_in_history(e)
        finally:
            if hasattr(self, 'signal_handler') and self.signal_handler: 
                self.signal_handler.unregister()
            
            await self._log_final_status_and_telemetry()


            if self.register_done_callback and self.state.status in [AgentStatus.COMPLETED, AgentStatus.FAILED, AgentStatus.MAX_STEPS_REACHED, AgentStatus.STOPPED]:
                 await self._try_run_callback_simple(self.register_done_callback, self.state.history)

            await self.close() 

            self._generate_final_gif_if_enabled()
        
        return self.state.history

    def _generate_final_gif_if_enabled(self): 
        if not self.settings.generate_gif:
            return
            
        if not self.state.history.history:
            agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, 
                     "Cannot generate GIF: No history data available.")
            return
            
        try:
            has_screenshots = False
            for history_item in self.state.history.history:
                if (hasattr(history_item, 'browser_state') and 
                    history_item.browser_state and 
                    hasattr(history_item.browser_state, 'screenshot') and 
                    history_item.browser_state.screenshot):
                    has_screenshots = True
                    break
                    
            if not has_screenshots:
                agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, 
                         "Cannot generate GIF: No valid screenshots found in agent history.")
                return
                
            output_path_str = self.settings.generate_gif if isinstance(self.settings.generate_gif, str) else f"agent_run_{self.state.agent_id}.gif"
            output_path = Path(output_path_str).resolve()
            output_path.parent.mkdir(parents=True, exist_ok=True)

            agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, f"Generating execution GIF to: {output_path}")
            
            create_history_gif(
                task=self.state.task.get('instructions', 'Agent Task'), 
                history=self.state.history,
                output_path=str(output_path)
            )
            agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, f"GIF successfully generated: {output_path}")

        except ImportError:
            agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, 
                     "Pillow (PIL) library not installed. Cannot generate GIF. Please install it (`pip install Pillow`).")
        except (AttributeError, TypeError) as e: 
            agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps,
                      f"Data structure error during GIF generation (e.g., history item or browser_state malformed): {e}", exc_info=True)
        except Exception as e: 
            agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps,
                      f"Unexpected error during GIF generation: {e}", exc_info=True)
        
    async def _record_error_in_history(self, error: Exception) -> None: # Changed from def to async def
        if not self.state: 
            logger.error(f"Attempted to record error but agent state is not available. Error: {error}")
            return

        error_action_result = self.controller._create_action_result( # Corrected: use controller's method
            action_name="runtime_error_event", 
            parameters={"error_type": type(error).__name__, "error_message": str(error)[:500]},
            success=False,
            error_message=f"Runtime Error Encountered: {str(error)[:500]}", 
            include_in_memory=False 
        )
        
        current_browser_state_for_history: Optional[BrowserStateHistory] = None
        try:
            if self.browser_context and self.browser_context.is_running(): 
                live_browser_state = await self.browser_context.get_state(cache_clickable_elements_hashes=False) # Corrected: await
                current_browser_state_for_history = BrowserStateHistory(
                    url=live_browser_state.url,
                    title=live_browser_state.title,
                    tabs=live_browser_state.tabs,
                    interacted_element=[], 
                    screenshot=live_browser_state.screenshot
                )
        except Exception as bs_error:
            agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, f"Could not capture browser state during error recording: {bs_error}")

        history_item = AgentHistory(
            step=self.state.n_steps, 
            browser_state=current_browser_state_for_history,
            agent_output=None, 
            action_results=[error_action_result],
            metadata=StepMetadata(
                step_number=self.state.n_steps,
                step_start_time=time.monotonic(), 
                step_end_time=time.monotonic(),
            )
        )
        self.state.history.history.append(history_item)
        agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, f"Recorded runtime error to agent history: {type(error).__name__}")

    async def _placeholder_load_relevant_context_from_db(self, task_info: Dict[str, Any]) -> Optional[str]:
        agent_log(logging.INFO, self.state.agent_id, 0, f"Placeholder: Attempting to load initial summary from DB for task: {task_info.get('instructions', 'N/A')[:50]}...")
        return None

    async def _placeholder_save_final_summary_to_db(self, task_info: Dict[str, Any], summary: str) -> None:
        agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, f"Placeholder: Saving final summary to DB for task: {task_info.get('id', 'N/A')}. Summary: {summary[:100]}...")
        pass

    async def _validate_output(self) -> bool: # Changed from placeholder
        if not self.settings.validate_output:
            agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, "Output validation skipped (disabled in settings).")
            return True 

        if not self.llm: 
            agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, "Output validation enabled, but no LLM available for it. Skipping.")
            return True 

        agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "Performing output validation...")
        
        task_instr = self.state.task.get('instructions', 'N/A')
        current_strat_goal = self.state.current_goal or 'N/A'
        last_results_str = json.dumps([res.model_dump(exclude_none=True) for res in self.state.last_result] if self.state.last_result else "No actions in last step.", indent=2, ensure_ascii=False)

        system_msg_content = (
            f"You are an impartial Output Validation Assistant. Your task is to evaluate if the agent's last set of actions "
            f"and their results align with the overall task and current strategic goal. "
            f"Overall Task: \"{task_instr[:500]}...\"\n"
            f"Agent's Current Strategic Goal for the last step: \"{current_strat_goal[:500]}...\"\n"
            f"Agent's last executed action(s) and their results were:\n```json\n{last_results_str[:2000]}...\n```\n"
            f"The current browser page screenshot is also provided (if vision is enabled by the main agent).\n"
            f"Based on ALL this information, determine if the outcome seems valid and contributes positively towards the goals. "
            f"Respond ONLY with a JSON object adhering to this schema: {{\"is_valid\": boolean, \"reason\": \"string detailing your assessment and suggestions if invalid\"}}."
        )
        
        messages_for_validation: List[BaseMessage] = [SystemMessage(content=system_msg_content)]
        
        if self.browser_context and self.browser_context.is_running():
            try:
                validation_browser_state = await self.browser_context.get_state(cache_clickable_elements_hashes=False)
                obs_text = (
                    f"Current URL for validation: {validation_browser_state.url}\n"
                    f"Page Title: {validation_browser_state.title}"
                )
                obs_content_list: List[Union[str, Dict[str,Any]]] = [{"type": "text", "text": obs_text}]
                if self.settings.use_vision and validation_browser_state.screenshot: 
                    obs_content_list.append({"type": "image_url", "image_url": {"url": f"data:image/png;base64,{validation_browser_state.screenshot}"}})
                messages_for_validation.append(HumanMessage(content=obs_content_list))
            except Exception as e_val_bs:
                agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, f"Could not get browser state for validation prompt: {e_val_bs}")
        
        class ValidationResponseSchema(BaseModel):
            is_valid: bool
            reason: str

        try:
            validation_llm_call_start_time = time.monotonic()
            if hasattr(self.llm, 'with_structured_output'):
                 temp_validator_llm = self.llm.with_structured_output(ValidationResponseSchema, method=self.settings.tool_calling_method or "json_mode") 
                 validator_response_obj = await temp_validator_llm.ainvoke(messages_for_validation)
                 parsed_validation = validator_response_obj 
            else: 
                raw_response_val = await self.llm.ainvoke(messages_for_validation)
                parsed_dict_val = extract_json_from_model_output(str(raw_response_val.content))
                parsed_validation = ValidationResponseSchema.model_validate(parsed_dict_val)

            agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, f"Validation LLM call took {time.monotonic() - validation_llm_call_start_time:.2f}s.")

            if not parsed_validation.is_valid:
                reason = parsed_validation.reason or "Validation indicated output is not valid."
                agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, f"Output VALIDATION FAILED. Reason: {reason}")
                self.state.last_error = f"OutputValidationFailed: {reason}"
                self.state.last_result = [ActionResult(action_name="output_validation_check", success=False, error=self.state.last_error, extracted_content=f"Validation: {reason}")]
                if self.state.status not in [AgentStatus.COMPLETED, AgentStatus.FAILED, AgentStatus.STOPPED, AgentStatus.PAUSED]:
                    self.state.status = AgentStatus.RECOVERING
            else:
                agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, f"Output VALIDATION PASSED. Reason: {parsed_validation.reason}")
            return parsed_validation.is_valid

        except Exception as e_val:
            agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps, f"Error during output validation process: {e_val}", exc_info=True)
            return True 

    async def _try_run_callback_simple(self, callback: Optional[Callable[..., Awaitable[None]]], *args):
        if callback:
            callback_name = getattr(callback, '__name__', 'UnnamedCallback')
            try:
                await callback(*args)
                agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, f"Callback '{callback_name}' executed successfully.")
            except Exception as e:
                agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps,
                          f"Error executing callback '{callback_name}': {e}", exc_info=True)

    def _try_save_conversation_step_data(self, step_num_for_filename: int, history_item: AgentHistory) -> None:
        if self.settings.save_conversation_path:
            try:
                save_dir = Path(self.settings.save_conversation_path) 
                filename = f"agent_{self.state.agent_id}_step_{step_num_for_filename:03d}_history.json"
                filepath = save_dir / filename
                
                save_conversation_step_to_file(
                    history_item.model_dump(mode='json', exclude_none=True), 
                    str(filepath), 
                    encoding=self.settings.save_conversation_path_encoding
                )
            except Exception as save_err: 
                agent_log(logging.ERROR, self.state.agent_id, history_item.step,
                          f"Failed to save conversation step {step_num_for_filename} data to file: {save_err}", exc_info=True)
    
    def _log_agent_run_start_telemetry(self) -> None:
        agent_log(logging.INFO, self.state.agent_id, 0, f"ðŸš€ Starting agent run for task: \"{self.state.task.get('instructions', 'N/A')[:70]}...\"")
        self.telemetry.capture(AgentRunTelemetryEvent( # Corrected from self.telemetry.capture
            agent_id=self.state.agent_id,
            use_vision=self.settings.use_vision,
            task=self.state.task.get('instructions', 'N/A'),
            model_name=self.model_name,
            chat_model_library=self.chat_model_library,
            version=self.version,
            source=self.source,
        ))

    async def _log_final_status_and_telemetry(self):
        final_message = self.state.accumulated_output or f"Run ended with status: {self.state.status.value}"
        log_level = logging.INFO
        if self.state.status == AgentStatus.FAILED: log_level = logging.ERROR
        elif self.state.status == AgentStatus.MAX_STEPS_REACHED: log_level = logging.WARNING
        
        agent_log(log_level, self.state.agent_id, self.state.n_steps, 
                  f"ðŸ Agent run finished. Final Status: {self.state.status.value}. Steps taken: {self.state.n_steps}. Output: {final_message[:200]}...")
        
        errors_list = [
            err_item.error 
            for hist_item in self.state.history.history 
            for err_item in hist_item.action_results 
            if err_item.error and not err_item.success
        ]

        self.telemetry.capture(AgentEndTelemetryEvent( # Corrected from self.telemetry.capture
            agent_id=self.state.agent_id,
            steps=self.state.n_steps,
            max_steps_reached=(self.state.status == AgentStatus.MAX_STEPS_REACHED),
            is_done=self.state.history.is_done(),
            success=self.state.history.is_successful(),
            total_input_tokens=sum(item.metadata.input_tokens for item in self.state.history.history if item.metadata and item.metadata.input_tokens is not None),
            total_duration_seconds=self.state.history.total_duration_seconds(),
            errors=errors_list[:10] # Send up to 10 errors
        ))

    async def _save_strategy_to_file(self, task_id: str, strategy: str, current_step: int) -> None:
        if not self.settings.save_learned_strategies:
            return
        try:
            sane_task_id = re.sub(r'[^\w\-_\.]', '_', task_id[:50]) 
            filename = f"strategy_{sane_task_id}_step{current_step}.md"
            
            strategy_content = (
                f"# Effective Strategy Identified\n\n"
                f"## Task ID: {task_id}\n"
                f"## Identified at Step: {current_step}\n"
                f"## Timestamp: {datetime.now().isoformat()}\n\n" 
                f"## Strategy:\n\n{strategy}\n"
            )
            await pc_tool_write_agent_text_file(filename=filename, content=strategy_content, append=False)
            agent_log(logging.INFO, self.state.agent_id, current_step, f"Saved effective strategy to Agent_Dir/{filename}")
        except LocalPCToolError as e: 
            agent_log(logging.ERROR, self.state.agent_id, current_step, f"Failed to save strategy to file in Agent_Dir: {e}")
        except Exception as e: 
            agent_log(logging.ERROR, self.state.agent_id, current_step, f"Unexpected error saving strategy: {e}", exc_info=True)

    async def _on_exit_signal(self):
        agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "Exit signal received by agent. Initiating stop sequence.")
        if self.state.status not in [AgentStatus.STOPPED, AgentStatus.FAILED, AgentStatus.COMPLETED]:
            self.state.status = AgentStatus.STOPPED 
            self.state.accumulated_output = self.state.accumulated_output or "Agent run terminated by signal."

    @time_execution_async('--step (agent)')
    async def step(self, step_info: Optional[AgentStepInfo] = None) -> Tuple[bool, bool]:
        current_step_num_for_logging = self.state.n_steps 
        agent_log(logging.INFO, self.state.agent_id, current_step_num_for_logging, 
                  f"Starting step {current_step_num_for_logging + 1}/{self.state.max_steps}. Current Status: {self.state.status.value}")

        browser_state_observed_for_llm: Optional[BrowserState] = None
        llm_output_for_this_step: Optional[AgentOutput] = None
        action_results_for_this_step: List[ActionResult] = [] 
        
        step_start_time = time.monotonic()
        prompt_tokens_for_llm_call = 0
        completion_tokens_from_llm = None 
        total_tokens_for_llm_call = None 

        try:
            if self.state.status == AgentStatus.STOPPED:
                agent_log(logging.INFO, self.state.agent_id, current_step_num_for_logging, "Step processing halted: Agent STOPPED.")
                raise AgentInterruptedError("Agent run stopped.")
            if self.state.status == AgentStatus.PAUSED:
                agent_log(logging.INFO, self.state.agent_id, current_step_num_for_logging, "Step processing halted: Agent PAUSED.")
                raise AgentInterruptedError("Agent run paused.")


            browser_state_observed_for_llm = await self.browser_context.get_state(cache_clickable_elements_hashes=True)
            active_page_obj = await self.browser_context.get_current_page()

            browser_state_hash_before_llm = None 
            try:
                element_tree_hash_str = ""
                if browser_state_observed_for_llm.element_tree and hasattr(browser_state_observed_for_llm.element_tree, 'hash'):
                    element_tree_hash_str = str(browser_state_observed_for_llm.element_tree.hash)
                
                browser_state_representation_pre_llm = browser_state_observed_for_llm.url + element_tree_hash_str
                browser_state_hash_before_llm = hash(json.dumps(browser_state_representation_pre_llm))
                logger.debug(f"Browser state hash before LLM call: {browser_state_hash_before_llm}", extra={'agent_id': self.state.agent_id, 'step': current_step_num_for_logging})
            except Exception as e_hash_pre:
                logger.warning(f"Could not create pre-LLM browser state hash: {e_hash_pre}", extra={'agent_id': self.state.agent_id, 'step': current_step_num_for_logging})
            
            # Placeholder for memory creation logic if it existed
            # if hasattr(self.settings, 'enable_memory') ...

            if self.state.status == AgentStatus.STOPPED or self.state.status == AgentStatus.PAUSED: # Recheck after potential async ops
                raise AgentInterruptedError(f"Agent status changed to {self.state.status.value} before LLM call.")


            await self._update_action_models_for_page(active_page_obj)
            page_specific_actions_description = self.controller.registry.get_prompt_description(page=active_page_obj)
            current_step_info_obj = step_info or AgentStepInfo(step_number=self.state.n_steps, max_steps=self.state.max_steps)

            if self.settings.use_planner and self.planner_llm and \
               (self.state.n_steps == 0 or \
                (self.settings.planner_interval > 0 and self.state.n_steps % self.settings.planner_interval == 0) or \
                (self.settings.reflect_on_error and self.state.last_error is not None)):
                agent_log(logging.INFO, self.state.agent_id, current_step_num_for_logging, "Running reflector/planner...")
                planner_reflection_output = await self._run_planner(current_browser_state_for_planner=browser_state_observed_for_llm)
                if planner_reflection_output: 
                    if isinstance(planner_reflection_output, ReflectionPlannerOutput):
                        self.state.current_goal = planner_reflection_output.next_goal
                        self.state.current_memory_summary = planner_reflection_output.memory_summary
                        agent_log(logging.INFO, self.state.agent_id, current_step_num_for_logging, f"Planner updated. New Goal: '{self.state.current_goal}'. Summary Updated.")
                        if self.settings.save_learned_strategies and planner_reflection_output.effective_strategy:
                             await self._save_strategy_to_file(self.state.task.get('id', self.state.agent_id), planner_reflection_output.effective_strategy, current_step_num_for_logging)
                        if self.settings.reflect_on_error and self.state.last_error: 
                            self.state.last_error = None; self.state.consecutive_failures = 0 
                    else: 
                        self.state.current_goal = str(planner_reflection_output) 
                        agent_log(logging.INFO, self.state.agent_id, current_step_num_for_logging, f"Planner provided new goal (string): '{self.state.current_goal}'.")

            llm_input_messages = self._message_manager.prepare_messages(
                agent_history_for_prompt=self.state.history.history,
                task_instructions=self.state.task.get('instructions', 'Undefined task'),
                current_browser_state_for_prompt=browser_state_observed_for_llm,
                current_goal=self.state.current_goal,
                current_memory_summary=self.state.current_memory_summary,
                agent_status=self.state.status, agent_id=self.state.agent_id,
                last_error=self.state.last_error, 
                page_specific_actions_desc=page_specific_actions_description,
                use_vision_for_current_state=self.settings.use_vision
            )
            
            if hasattr(self._message_manager.state.history, 'current_tokens'):
                 prompt_tokens_for_llm_call = self._message_manager.state.history.current_tokens 
            else: 
                 prompt_tokens_for_llm_call = sum(m.metadata.tokens for m in self._message_manager.state.history.messages)

            output_schema_for_llm = self.AgentOutput
            if current_step_info_obj.is_last_step():
                agent_log(logging.INFO, self.state.agent_id, current_step_num_for_logging, "Last step: using DoneAgentOutput schema.")
                output_schema_for_llm = self.DoneAgentOutput
                llm_input_messages.append(HumanMessage(content="CRITICAL ADVISORY: This is your final step. You MUST use the 'done' action. Evaluate task success accurately."))

            try:
                llm_output_for_this_step = await self.get_next_action(llm_input_messages, output_schema_for_llm)
                
                if hasattr(self.llm, '_last_completion_tokens') and getattr(self.llm, '_last_completion_tokens', None) is not None:
                    completion_tokens_from_llm = self.llm._last_completion_tokens # type: ignore
                    if hasattr(self.llm, '_last_total_tokens') and getattr(self.llm, '_last_total_tokens', None) is not None:
                        total_tokens_for_llm_call = self.llm._last_total_tokens # type: ignore
                        if prompt_tokens_for_llm_call is None and total_tokens_for_llm_call is not None and completion_tokens_from_llm is not None:
                            prompt_tokens_for_llm_call = total_tokens_for_llm_call - completion_tokens_from_llm
                    elif prompt_tokens_for_llm_call is not None and completion_tokens_from_llm is not None:
                         total_tokens_for_llm_call = prompt_tokens_for_llm_call + completion_tokens_from_llm
                elif hasattr(llm_output_for_this_step, 'response_metadata') and llm_output_for_this_step.response_metadata: # type: ignore
                    usage = llm_output_for_this_step.response_metadata.get('token_usage') or llm_output_for_this_step.response_metadata.get('usage') # type: ignore
                    if isinstance(usage, dict):
                        completion_tokens_from_llm = usage.get('completion_tokens') or usage.get('output_tokens')
                        llm_provided_prompt_tokens = usage.get('prompt_tokens') or usage.get('input_tokens')
                        if llm_provided_prompt_tokens is not None:
                            prompt_tokens_for_llm_call = llm_provided_prompt_tokens
                        total_tokens_for_llm_call = usage.get('total_tokens')
                
                if llm_output_for_this_step: 
                    log_agent_output_structured(llm_output_for_this_step, self.state.agent_id, current_step_num_for_logging, self) # Added self for agent_instance
                
                if self.state.status == AgentStatus.STOPPED or self.state.status == AgentStatus.PAUSED: # Recheck after LLM
                    raise AgentInterruptedError(f"Agent status changed to {self.state.status.value} after LLM call.")

                
                if browser_state_hash_before_llm is not None: 
                    try:
                        current_browser_state_after_llm = await self.browser_context.get_state(cache_clickable_elements_hashes=False) 
                        element_tree_hash_str_post_llm = ""
                        if current_browser_state_after_llm.element_tree and hasattr(current_browser_state_after_llm.element_tree, 'hash'):
                            element_tree_hash_str_post_llm = str(current_browser_state_after_llm.element_tree.hash)
                        browser_state_representation_post_llm = current_browser_state_after_llm.url + element_tree_hash_str_post_llm
                        current_hash_after_llm = hash(json.dumps(browser_state_representation_post_llm))
                        logger.debug(f"Browser state hash after LLM call: {current_hash_after_llm}", extra={'agent_id': self.state.agent_id, 'step': current_step_num_for_logging})

                        if browser_state_hash_before_llm != current_hash_after_llm:
                            warning_msg = "Browser state changed during LLM deliberation. Actions planned on stale state will be aborted. Agent should re-plan."
                            agent_log(logging.WARNING, self.state.agent_id, current_step_num_for_logging, warning_msg)
                            llm_output_for_this_step = None 
                            action_results_for_this_step = [self.controller._create_action_result( # Use controller's helper
                                action_name="state_change_during_deliberation",
                                parameters={"original_hash": browser_state_hash_before_llm, "new_hash": current_hash_after_llm},
                                success=False, error_message=warning_msg, # error_message instead of error
                                extracted_data="Browser state changed, planned actions aborted.", # extracted_data instead of extracted_content
                                include_in_memory=True )]
                    except Exception as e_hash_post:
                        agent_log(logging.WARNING, self.state.agent_id, current_step_num_for_logging, f"Could not create/compare post-LLM browser state hash: {e_hash_post}. Proceeding with caution.")
            except asyncio.CancelledError: 
                raise AgentInterruptedError('Model query cancelled by user') # Changed from InterruptedError
            
            if not action_results_for_this_step: 
                if llm_output_for_this_step and llm_output_for_this_step.action:
                    action_results_for_this_step = await self.multi_act( # Changed from self.controller.multi_act
                        actions_from_llm_model_instances=llm_output_for_this_step.action, 
                        browser_context=self.browser_context,
                        page_extraction_llm=self.settings.page_extraction_llm,
                        sensitive_data=self.sensitive_data,
                        available_file_paths=self.settings.available_file_paths,
                        context=self.context
                    )
                else: 
                    agent_log(logging.INFO, self.state.agent_id, current_step_num_for_logging, "LLM planned no actions for this step.")
                    action_results_for_this_step = [ActionResult(action_name="no_actions_planned", success=True, extracted_content="No actions planned by LLM for this step.")]

            self.state.last_result = action_results_for_this_step

            is_task_completed_this_step = any(ar.is_done for ar in action_results_for_this_step)
            if is_task_completed_this_step:
                done_action = next((ar for ar in action_results_for_this_step if ar.is_done), None)
                if done_action: 
                    self.state.accumulated_output = done_action.extracted_content or ("Task completed " + ("successfully." if done_action.success else " (marked as not successful)."))
                    if done_action.success:
                        self.state.status = AgentStatus.COMPLETED
                        agent_log(logging.INFO, self.state.agent_id, current_step_num_for_logging, f"Task COMPLETED successfully: {self.state.accumulated_output}")
                    else:
                        self.state.status = AgentStatus.COMPLETED 
                        agent_log(logging.WARNING, self.state.agent_id, current_step_num_for_logging, f"Task COMPLETED (marked UNSUCCESSFUL by agent): {self.state.accumulated_output}")
                else: 
                    self.state.status = AgentStatus.COMPLETED 
                    self.state.accumulated_output = self.state.accumulated_output or "Task marked done, but specific outcome details missing."
                    agent_log(logging.WARNING, self.state.agent_id, current_step_num_for_logging, self.state.accumulated_output)
            
            if any(not ar.success for ar in action_results_for_this_step):
                first_error = next((ar.error for ar in action_results_for_this_step if ar.error), "An action in the step failed without specific error message.")
                self.state.last_error = first_error
                # Using direct increment and log as _increment_failures_and_log is not in this class
                self.state.consecutive_failures += 1
                agent_log(logging.WARNING, self.state.agent_id, current_step_num_for_logging, 
                          f"Step resulted in failure. Error: {first_error}. Consecutive failures: {self.state.consecutive_failures}")
                if self.state.status not in [AgentStatus.COMPLETED, AgentStatus.FAILED, AgentStatus.STOPPED, AgentStatus.PAUSED]:
                    self.state.status = AgentStatus.RECOVERING 
            else: 
                self.state.consecutive_failures = 0
                self.state.last_error = None 

        except AgentInterruptedError as e: # Changed from InterruptedError to AgentInterruptedError
            agent_log(logging.INFO, self.state.agent_id, current_step_num_for_logging, f"Step interrupted: {e}.")
            action_results_for_this_step = self.state.last_result or [self.controller._create_action_result(action_name="step_interrupted", parameters=None, success=False, error_message=str(e))] # Use controller's helper
        except (TimeoutError, asyncio.TimeoutError) as e: 
             step_timeout_duration = getattr(self.settings, 'step_timeout', 300) 
             timeout_msg = f"Step {current_step_num_for_logging + 1} timed out after {step_timeout_duration}s."
             agent_log(logging.ERROR, self.state.agent_id, current_step_num_for_logging, timeout_msg + f" Details: {e}", exc_info=True)
             self.state.last_error = timeout_msg
             self.state.consecutive_failures +=1
             agent_log(logging.WARNING, self.state.agent_id, current_step_num_for_logging, 
                       f"Step resulted in failure (timeout). Consecutive failures: {self.state.consecutive_failures}")
             if self.state.status not in [AgentStatus.COMPLETED, AgentStatus.FAILED, AgentStatus.STOPPED, AgentStatus.PAUSED]: self.state.status = AgentStatus.RECOVERING
             action_results_for_this_step = [self.controller._create_action_result(action_name="step_timeout", parameters=None, success=False, error_message=timeout_msg)] # Use controller's helper
        except (LLMCommunicationError, LLMOutputParsingError, LLMException, AgentException) as e: 
            agent_log(logging.ERROR, self.state.agent_id, current_step_num_for_logging, f"Agent/LLM Exception in step: {e}", exc_info=True)
            action_results_for_this_step = await self._handle_step_error(e) 
        except Exception as e: 
            agent_log(logging.CRITICAL, self.state.agent_id, current_step_num_for_logging, f"Unhandled error in step: {e}", exc_info=True)
            action_results_for_this_step = await self._handle_step_error(e) 
        finally:
            step_end_time = time.monotonic()
            final_browser_state_for_history: Optional[BrowserStateHistory] = None
            final_browser_state_snapshot_for_history: Optional[BrowserState] = browser_state_observed_for_llm 
            
            actions_were_attempted = not (action_results_for_this_step and len(action_results_for_this_step) > 0 and action_results_for_this_step[0].action_name == "state_change_during_deliberation")
            if actions_were_attempted and hasattr(self, 'browser_context') and self.browser_context.is_running():
                try:
                    final_browser_state_snapshot_for_history = await self.browser_context.get_state(cache_clickable_elements_hashes=False)
                except Exception as e_final_state:
                    agent_log(logging.WARNING, self.state.agent_id, current_step_num_for_logging, f"Could not get final browser state after actions, using pre-action state for history: {e_final_state}")
            
            if final_browser_state_snapshot_for_history: 
                interacted_elements = self._determine_interacted_elements(
                    agent_output=llm_output_for_this_step, 
                    current_browser_state=final_browser_state_snapshot_for_history )
                final_browser_state_for_history = BrowserStateHistory(
                    url=final_browser_state_snapshot_for_history.url,
                    title=final_browser_state_snapshot_for_history.title,
                    tabs=final_browser_state_snapshot_for_history.tabs,
                    interacted_element=interacted_elements,
                    screenshot=final_browser_state_snapshot_for_history.screenshot )
            
            metadata_args = {
                "step_number": current_step_num_for_logging,
                "step_start_time": step_start_time,
                "step_end_time": step_end_time,
                "input_tokens": prompt_tokens_for_llm_call,
                "completion_tokens": completion_tokens_from_llm,
                "total_tokens": total_tokens_for_llm_call,
                "prompt_tokens": prompt_tokens_for_llm_call 
            }
            step_metadata = StepMetadata(**{k: v for k, v in metadata_args.items() if v is not None})

            self._make_history_item( 
                model_output=llm_output_for_this_step, 
                observed_browser_state_for_item=final_browser_state_snapshot_for_history,
                action_results=action_results_for_this_step, 
                metadata=step_metadata )

            agent_log(logging.INFO, self.state.agent_id, current_step_num_for_logging,
                      f"Step {current_step_num_for_logging + 1} processed. Duration: {step_metadata.duration_seconds:.2f}s. Current Agent Status: {self.state.status.value if self.state.status else 'N/A'}.")
            
            if self.register_new_step_callback and self.state.history.history:
                 await self._try_run_callback_simple(self.register_new_step_callback, self.state.history.history[-1])

            if self.settings.save_conversation_path and self.state.history.history:
                self._try_save_conversation_step_data(current_step_num_for_logging + 1, self.state.history.history[-1])

            self.state.n_steps += 1 
            
            step_is_done_final = self.state.status == AgentStatus.COMPLETED 
            task_is_successful_if_done_final = False 
            if step_is_done_final:
                 last_done_action = next((ar for ar in action_results_for_this_step if ar.is_done), None)
                 if last_done_action: 
                     task_is_successful_if_done_final = last_done_action.success
                 elif not any(not ar.success for ar in action_results_for_this_step) and not self.state.last_error:
                     task_is_successful_if_done_final = True

            step_is_valid_final = self.state.status not in [AgentStatus.FAILED, AgentStatus.STOPPED]
            return task_is_successful_if_done_final, step_is_valid_final
            
    def _determine_interacted_elements(
        self,
        agent_output: Optional[AgentOutput], 
        current_browser_state: BrowserState, 
    ) -> List[Optional[DOMHistoryElement]]:
        if not agent_output or not agent_output.action:
            return [] 
            
        include_dyn_attrs: bool = self.browser_context.config.include_dynamic_attributes
                      
        interacted_dom_history_elements: List[Optional[DOMHistoryElement]] = []
        selector_map = current_browser_state.selector_map

        for action_model_instance in agent_output.action:
            action_dump = action_model_instance.model_dump(exclude_unset=True)
            
            if not action_dump or len(action_dump) != 1:
                interacted_dom_history_elements.append(None) 
                continue

            action_name = next(iter(action_dump))
            action_params = action_dump[action_name]
            targeted_element_node: Optional[DOMElementNode] = None 

            if isinstance(action_params, dict) and 'index' in action_params:
                try:
                    target_index = int(action_params['index'])
                    if target_index in selector_map:
                        targeted_element_node = selector_map[target_index]
                    else:
                        agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps,
                                  f"Action '{action_name}' targeted index {target_index}, not found in current selector map for history recording.")
                except (ValueError, TypeError):
                    agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps,
                              f"Action '{action_name}' had invalid 'index' parameter for history: {action_params.get('index')}")
            
            if targeted_element_node:
                try:
                    from browser_use.dom.history_tree_processor.service import HistoryTreeProcessor 
                    history_el = HistoryTreeProcessor.convert_dom_element_to_history_element(
                        targeted_element_node # Removed include_dyn_attrs as it's not a param for this method
                    )
                    interacted_dom_history_elements.append(history_el)
                except Exception as e:
                    agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps,
                              f"Failed to convert DOMElementNode to DOMHistoryElement for action '{action_name}' (history): {e}", exc_info=True)
                    interacted_dom_history_elements.append(None)
            else:
                interacted_dom_history_elements.append(None)
                
        return interacted_dom_history_elements


    async def _execute_observe_think_act(
        self, 
        step_num: int, 
        current_browser_observation: BrowserState 
    ) -> Tuple[Optional[AgentOutput], List[ActionResult]]:
        agent_log(logging.DEBUG, self.state.agent_id, step_num, "Executing Observe-Think-Act sub-step.")
        
        active_page_details = await self.browser_context.get_current_page() 
        await self._update_action_models_for_page(active_page_details) 

        page_specific_actions_description = self.controller.registry.get_prompt_description(page=active_page_details)
        
        llm_input_messages: List[BaseMessage] = self._message_manager.prepare_messages(
             agent_history_for_prompt=self.state.history.history,
             task_instructions=self.state.task.get('instructions', 'Undefined task'),
             current_browser_state_for_prompt=current_browser_observation, 
             current_goal=self.state.current_goal,
             current_memory_summary=self.state.current_memory_summary,
             agent_status=self.state.status, 
             agent_id=self.state.agent_id, 
             last_error=None, 
             page_specific_actions_desc=page_specific_actions_description,
             use_vision_for_current_state=self.settings.use_vision
        )

        step_info_obj = AgentStepInfo(step_number=step_num, max_steps=self.state.max_steps)
        current_llm_output_schema_type = self.AgentOutput 
        if step_info_obj.is_last_step():
            agent_log(logging.INFO, self.state.agent_id, step_num, "This is the last allowed step. Forcing 'done' action schema.")
            current_llm_output_schema_type = self.DoneAgentOutput 
            last_step_instruction_msg = HumanMessage(content=(
                "CRITICAL ADVISORY: You are on your final allowed step. Your response MUST use the 'done' action. "
                "Evaluate the overall task success and provide a comprehensive final answer or summary in the 'text' parameter of the 'done' action. "
                "Set the 'success' parameter of the 'done' action accurately."
            ))
            llm_input_messages.append(last_step_instruction_msg)

        agent_llm_output: AgentOutput = await self.get_next_action(llm_input_messages, current_llm_output_schema_type) # Changed from agent_llm_output_dict
        log_agent_output_structured(agent_llm_output, self.state.agent_id, step_num, self) # Added self for agent_instance

        if self.state.status == AgentStatus.STOPPED or self.state.status == AgentStatus.PAUSED: # Recheck before acting
            raise AgentInterruptedError(f"Agent status changed to {self.state.status.value} before acting.")

        action_model_instances_to_execute: List[ActionModel] = agent_llm_output.action
        
        executed_action_results: List[ActionResult] = await self.multi_act( # Changed from self.controller.multi_act
            actions_from_llm_model_instances=action_model_instances_to_execute, 
            browser_context=self.browser_context, # Pass the correct context
            page_extraction_llm=self.settings.page_extraction_llm,
            sensitive_data=self.sensitive_data,
            available_file_paths=self.settings.available_file_paths,
            context=self.context
        )
        self.state.last_result = executed_action_results 

        return agent_llm_output, executed_action_results

    @time_execution_async('--handle_step_error (agent_service)')
    async def _handle_step_error(self, error: Exception) -> List[ActionResult]:
        error_msg_concise = f"{type(error).__name__}: {str(error)}"

        if logger.isEnabledFor(logging.DEBUG):
            detailed_error_msg_tb = "".join(traceback.format_exception(type(error), error, error.__traceback__))
            agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps,
                      f"Error during step: {error_msg_concise}\nTraceback:\n{detailed_error_msg_tb}")
        else:
            agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps,
                      f"Error during step: {error_msg_concise}")

        self.state.last_error = error_msg_concise
        self.state.consecutive_failures += 1
        agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps,
                    f"Step error incremented failures. Error: {error_msg_concise}. Consecutive: {self.state.consecutive_failures}")


        if self.state.consecutive_failures >= self.settings.max_failures:
            self.state.status = AgentStatus.FAILED
            self.state.accumulated_output = self.state.accumulated_output or f"Agent failed due to repeated errors. Last error: {error_msg_concise}"
            agent_log(logging.CRITICAL, self.state.agent_id, self.state.n_steps, "Max failures reached. Agent status set to FAILED.")
        elif self.state.status not in [AgentStatus.STOPPED, AgentStatus.PAUSED, AgentStatus.FAILED, AgentStatus.COMPLETED]:
            self.state.status = AgentStatus.RECOVERING
            agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "Transitioning to RECOVERING state due to step error.")

        return [self.controller._create_action_result(
            action_name="step_processing_error", 
            parameters={"error_type": type(error).__name__, "original_message": str(error)[:200]},
            success=False, 
            error_message=error_msg_concise, 
            include_in_memory=True 
        )]
    

    def _make_history_item(
        self,
        model_output: Optional[AgentOutput],
        observed_browser_state_for_item: Optional[BrowserState], 
        action_results: List[ActionResult], 
        metadata: Optional[StepMetadata] = None,
    ) -> None:
        current_action_results = action_results
        step_num_for_log = (metadata.step_number if metadata 
                            else self.state.n_steps if hasattr(self.state, 'n_steps') else 'Unknown')


        if not current_action_results: 
            agent_log(logging.WARNING, self.state.agent_id, step_num_for_log,
                      "No action results provided for history item. Creating placeholder error result.")
            current_action_results = [self.controller._create_action_result(
                action_name="empty_step_action_results",
                parameters={}, success=False,
                error_message="No action results were available for this step record."
            )]

        browser_state_for_history_record: Optional[BrowserStateHistory] = None
        if observed_browser_state_for_item:
            interacted_elements_list = self._determine_interacted_elements(
                agent_output=model_output, 
                current_browser_state=observed_browser_state_for_item
            )
            browser_state_for_history_record = observed_browser_state_for_item.to_history(
                interacted_elements=interacted_elements_list
            )

        if not browser_state_for_history_record:
            agent_log(logging.WARNING, self.state.agent_id, step_num_for_log,
                      "Browser state unavailable for history item. Creating minimal placeholder.")
            from browser_use.browser.views import BrowserStateHistory 
            browser_state_for_history_record = BrowserStateHistory( 
                url="unknown_url_at_history_point",
                title="State unavailable for this record",
                tabs=[], interacted_element=[] 
            )

        current_hist_step_num = step_num_for_log 
        final_metadata = metadata
        if final_metadata:
            final_metadata.step_number = current_hist_step_num if isinstance(current_hist_step_num, int) else self.state.n_steps
        else: 
            final_metadata = StepMetadata(
                step_number= current_hist_step_num if isinstance(current_hist_step_num, int) else self.state.n_steps,
                step_start_time=time.monotonic() -1, 
                step_end_time=time.monotonic(),
            )
        
        if hasattr(self, "_last_prompt_tokens_for_step") and self._last_prompt_tokens_for_step is not None:
             final_metadata.input_tokens = self._last_prompt_tokens_for_step
             final_metadata.prompt_tokens = self._last_prompt_tokens_for_step
        if hasattr(self, "_last_completion_tokens_for_step") and self._last_completion_tokens_for_step is not None:
             final_metadata.completion_tokens = self._last_completion_tokens_for_step
        if hasattr(self, "_last_total_tokens_for_step") and self._last_total_tokens_for_step is not None:
             final_metadata.total_tokens = self._last_total_tokens_for_step


        history_item = AgentHistory(
            step=final_metadata.step_number, 
            browser_state=browser_state_for_history_record,
            agent_output=model_output,
            action_results=current_action_results, 
            metadata=final_metadata
        )

        self.state.history.history.append(history_item)
        self.telemetry.capture(AgentStepTelemetryEvent( # Corrected from self.telemetry.capture
                agent_id=self.state.agent_id,
                step=history_item.step,
                step_error=[res.error for res in history_item.action_results if res.error and not res.success],
                consecutive_failures=self.state.consecutive_failures,
                actions=[
                    {
                        "name": action_res.action_name,
                        "params": action_res.parameters, # Parameters should be serializable if ActionResult is well-defined
                        "success": action_res.success,
                    }
                    for action_res in history_item.action_results
                ],
            ))


    THINK_TAGS_PATTERN = re.compile(r'<think>.*?</think>', re.DOTALL)
    STRAY_CLOSE_TAG_PATTERN = re.compile(r'.*?</think>', re.DOTALL)

    def _remove_think_tags(self, text: str) -> str: 
        text = re.sub(self.THINK_TAGS_PATTERN, '', text)
        text = re.sub(self.STRAY_CLOSE_TAG_PATTERN, '', text)
        return text.strip()

    def _convert_input_messages(self, input_messages: List[BaseMessage]) -> List[BaseMessage]:
        return convert_input_messages(input_messages, self.model_name)


    @time_execution_async('--get_next_action (agent_service)') 
    async def get_next_action(self, input_messages: List[BaseMessage], agent_output_schema: Type[AgentOutput]) -> AgentOutput: 
        messages_to_send_to_llm = self._convert_input_messages(input_messages)
        
        if not getattr(self.llm, '_verified_api_keys', False): 
             if not await self._verify_llm_connection(self.llm, 'Main LLM (get_next_action)'): 
                raise LLMCommunicationError("Main LLM connection issue before getting next action.")

        self._last_completion_tokens = None 
        self._last_total_tokens = None

        for attempt in range(MAX_LLM_RETRIES + 1):
            raw_llm_response_content = "" 
            try:
                agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, f"LLM call attempt {attempt + 1} for schema {agent_output_schema.__name__}")
                
                parsed_agent_output: Optional[AgentOutput] = None
                raw_llm_response: Any = None # To store the raw response for metadata
                
                llm_for_call = self.llm
                if self.settings.tool_calling_method and self.settings.tool_calling_method != 'raw': # Check if it's not 'raw' or None
                     try:
                         # Use the agent_output_schema passed to this function
                         llm_for_call = self.llm.with_structured_output(agent_output_schema, include_raw=True, method=self.settings.tool_calling_method)
                     except Exception as e_struct_setup:
                         agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, f"Failed to set up LLM with_structured_output for method '{self.settings.tool_calling_method}' and schema '{agent_output_schema.__name__}'. Falling back to raw. Error: {e_struct_setup}")
                         # Fallback to raw processing below by not reassigning llm_for_call or by setting a flag
                
                raw_llm_response = await llm_for_call.ainvoke(messages_to_send_to_llm)

                if isinstance(raw_llm_response, agent_output_schema): 
                    parsed_agent_output = raw_llm_response
                elif isinstance(raw_llm_response, dict) and 'parsed' in raw_llm_response and isinstance(raw_llm_response['parsed'], agent_output_schema):
                    parsed_agent_output = raw_llm_response['parsed']
                    if 'raw' in raw_llm_response and hasattr(raw_llm_response['raw'], 'response_metadata'):
                         raw_llm_response = raw_llm_response['raw'] # Use the raw message for metadata
                elif hasattr(raw_llm_response, 'tool_calls') and raw_llm_response.tool_calls: # type: ignore
                     agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, "LLM returned tool_calls; manual parsing from content will be attempted if content contains JSON.")
                     raw_llm_response_content = str(raw_llm_response.content) # type: ignore
                else: 
                    raw_llm_response_content = str(raw_llm_response.content) # type: ignore
                
                if parsed_agent_output is None:
                    if self.model_name and ("deepseek-r1" in self.model_name.lower() or "deepseek-reasoner" in self.model_name.lower()):
                        raw_llm_response_content = self._remove_think_tags(raw_llm_response_content)
                    
                    parsed_dict = extract_json_from_model_output(raw_llm_response_content) 
                    parsed_agent_output = agent_output_schema.model_validate(parsed_dict)
                
                if hasattr(raw_llm_response, 'response_metadata') and raw_llm_response.response_metadata: # type: ignore
                    usage = raw_llm_response.response_metadata.get('token_usage') or raw_llm_response.response_metadata.get('usage') # type: ignore
                    if isinstance(usage, dict):
                        self._last_completion_tokens_for_step = usage.get('completion_tokens') or usage.get('output_tokens') # Corrected to _last_completion_tokens_for_step
                        self._last_total_tokens_for_step = usage.get('total_tokens') # Corrected to _last_total_tokens_for_step


                if parsed_agent_output and len(parsed_agent_output.action) > self.settings.max_actions_per_step:
                    agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, f"LLM generated {len(parsed_agent_output.action)} actions, exceeding max {self.settings.max_actions_per_step}. Truncating.")
                    parsed_agent_output.action = parsed_agent_output.action[:self.settings.max_actions_per_step]

                # log_agent_output_structured is now called from step() after this method returns
                return parsed_agent_output 

            except (LLMOutputParsingError, ValidationError) as e:
                agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps, f"LLM output parsing/validation failed (attempt {attempt+1}): {e}. Raw content: {raw_llm_response_content[:200]}...")
                if attempt >= MAX_LLM_RETRIES:
                    raise LLMOutputParsingError(f"Failed to parse/validate LLM output after retries for {agent_output_schema.__name__}.", raw_output=raw_llm_response_content, original_exception=e) from e
                messages_to_send_to_llm.append(HumanMessage(content=f"Error in previous JSON output: {str(e)[:150]}. Ensure strict adherence to the schema."))
            except (LLMCommunicationError, LLMException, AgentException) as e:
                agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps, f"LLM communication error (attempt {attempt+1}): {e}", exc_info=True)
                if attempt >= MAX_LLM_RETRIES: raise
            except Exception as e:
                agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps, f"Unexpected LLM error (attempt {attempt+1}): {e}", exc_info=True)
                if attempt >= MAX_LLM_RETRIES: raise LLMException(f"LLM call failed after retries: {str(e)}") from e
            
            await asyncio.sleep(self.settings.retry_delay * (attempt + 1)) 
        
        raise LLMException("LLM call failed after all retries (should not be reached if logic is correct).")


    def _log_agent_run(self) -> None:
        # self.task in __init__ is a dict, get 'instructions' for task string
        task_instructions = self.state.task.get('instructions', 'N/A')
        agent_log(logging.INFO, self.state.agent_id, 0, f"ðŸš€ Starting Agent Run. Task: \"{task_instructions[:70]}...\"")
        agent_log(logging.DEBUG, self.state.agent_id, 0, f'Version: {self.version}, Source: {self.source}')
        self.telemetry.capture(
            AgentRunTelemetryEvent(
                agent_id=self.state.agent_id,
                use_vision=self.settings.use_vision,
                task=task_instructions, 
                model_name=self.model_name,
                chat_model_library=self.chat_model_library,
                version=self.version,
                source=self.source,
            )
        )

    async def take_step(self) -> Tuple[bool, bool]: 
        current_step_info = AgentStepInfo(step_number=self.state.n_steps, max_steps=self.state.max_steps)
        
        is_task_completed_and_successful_this_step, is_step_execution_valid_from_step = await self.step(current_step_info)

        is_step_execution_valid_overall = self.state.status not in [AgentStatus.FAILED, AgentStatus.STOPPED]
        
        if is_task_completed_and_successful_this_step:
            agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, 
                      f"take_step confirms: Task completed and successful at step {self.state.n_steps}.")
        elif self.state.status == AgentStatus.COMPLETED and not is_task_completed_and_successful_this_step:
            agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps,
                      f"take_step confirms: Task completed but marked unsuccessful at step {self.state.n_steps}.")
        
        if self.state.status == AgentStatus.COMPLETED and self.register_done_callback: 
            await self._try_run_callback_simple(self.register_done_callback, self.state.history)
            
        return is_task_completed_and_successful_this_step, is_step_execution_valid_overall


    @time_execution_async('--multi_act (agent_service)') # Corrected decorator to match definition
    async def multi_act(
        self,
        actions_from_llm_model_instances: List[ActionModel], 
        browser_context: BrowserContext, # Parameter name from definition
        page_extraction_llm: Optional[BaseChatModel] = None,
        sensitive_data: Optional[Dict[str, str]] = None,
        available_file_paths: Optional[List[str]] = None,
        context: ContextT | None = None,
    ) -> List[ActionResult]: 
        results: List[ActionResult] = []
        
        last_known_url: Optional[str] = None
        try:
            page_before_any_action = await browser_context.get_current_page()
            last_known_url = page_before_any_action.url
        except Exception as e:
            logger.warning(f"Could not get initial URL in multi_act: {e}")


        for i, action_model_instance in enumerate(actions_from_llm_model_instances):
            action_data = action_model_instance.model_dump(exclude_unset=True) 
            
            current_action_name = "unknown_action_in_sequence"
            current_params_dict: Dict[str, Any] = {}

            if not action_data or len(action_data) != 1: 
                logger.warning(f"Malformed ActionModel instance in sequence: {action_model_instance}. Skipping.")
                error_result = self.controller._create_action_result( # FIX: Use controller's method
                    action_name="malformed_action_model",
                    parameters={"malformed_action_data": str(action_model_instance)},
                    success=False,
                    error_message="Malformed action model instance received." # error_message instead of error
                )
                results.append(error_result)
                continue 

            try:
                current_action_name = next(iter(action_data.keys()))
                current_params_dict = action_data[current_action_name]
                if current_params_dict is None: current_params_dict = {} 

                log_params_display = current_params_dict
                if sensitive_data and current_action_name == "input_text" and "text" in current_params_dict and "<secret>" in current_params_dict["text"]:
                    log_params_display = {**current_params_dict, "text": "[SENSITIVE_DATA_PLACEHOLDER]"}
                
                logger.info(f"Executing action {i+1}/{len(actions_from_llm_model_instances)}: {current_action_name} with params: {log_params_display}")

                action_result_obj: ActionResult = await self.controller.registry.execute_action( # FIX: Access registry via controller
                    action_name=current_action_name,
                    params=current_params_dict, 
                    browser=browser_context, # Use the passed browser_context
                    page_extraction_llm=page_extraction_llm,
                    sensitive_data=sensitive_data,
                    available_file_paths=available_file_paths,
                    context=context
                )
                results.append(action_result_obj)

                try:
                    current_page_after_action = await browser_context.get_current_page()
                    last_known_url = current_page_after_action.url
                except Exception as e:
                    logger.warning(f"Could not get URL after action {current_action_name}: {e}")


                if action_result_obj.is_done or not action_result_obj.success:
                    log_msg = f"Action '{current_action_name}' "
                    log_msg += "marked task as done." if action_result_obj.is_done else f"failed (Error: {action_result_obj.error})."
                    log_msg += " Halting further actions in this step."
                    logger.info(log_msg)
                    break 
            
            except Exception as e:
                error_msg = f"Controller-level error executing action '{current_action_name}' in sequence: {str(e)}"
                logger.error(error_msg, exc_info=True)
                error_action_result = self.controller._create_action_result( # FIX: Use controller's method
                        action_name=current_action_name,
                        parameters=current_params_dict,
                        success=False,
                        error_message=error_msg # error_message instead of error
                    )
                results.append(error_action_result)
                break 
            
            if browser_context.config.wait_between_actions > 0 and i < len(actions_from_llm_model_instances) - 1:
                if not (results and (results[-1].is_done or not results[-1].success)):
                    logger.debug(f"Waiting for {browser_context.config.wait_between_actions}s before next action.")
                    
                    url_before_wait = last_known_url
                    
                    await asyncio.sleep(browser_context.config.wait_between_actions)

                    try:
                        page_after_wait = await browser_context.get_current_page()
                        url_after_wait = page_after_wait.url

                        if url_before_wait != url_after_wait:
                            logger.info(f"URL changed during action wait from '{url_before_wait}' to '{url_after_wait}', updating browser state before next action.")
                            await browser_context.get_state(cache_clickable_elements_hashes=True) 
                            last_known_url = url_after_wait 
                    except Exception as e:
                        logger.warning(f"Error checking/refreshing state during wait in multi_act: {e}")
        
        return results

    async def load_and_rerun(self, history_filepath: Union[str, Path] = "AgentHistory.json", **kwargs_for_rerun) -> List[ActionResult]:
        agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, f"Loading history from '{history_filepath}' for rerun.")
        history_to_replay = AgentHistoryList.load_from_file(history_filepath, agent_output_action_model=self.ActionModel)
        return await self.rerun_history(history_to_replay, **kwargs_for_rerun)

    def save_history(self, file_path: Union[str, Path] = "AgentHistory.json") -> None:
        if isinstance(self.state.history, AgentHistoryList):
            self.state.history.save_to_file(file_path) 
        else:
            agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps, "Cannot save history: self.state.history is not an AgentHistoryList.")

    async def rerun_history( # Added async as it calls async methods
        self,
        history_to_replay: AgentHistoryList,
        max_retries_per_step: int = 0, 
        skip_failures: bool = True, 
        delay_between_actions: float = 0.5, 
    ) -> List[ActionResult]: 
        agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, f"Starting RERUN of {len(history_to_replay.history)} history steps.")
        
        all_rerun_action_results_objects: List[ActionResult] = []
        initial_step_count_for_rerun = self.state.n_steps 

        for i, historical_step_item in enumerate(history_to_replay.history):
            current_actual_step_num = self.state.n_steps 
            step_start_time_rerun = time.monotonic() 

            historical_goal = historical_step_item.agent_output.current_state.next_goal if historical_step_item.agent_output else 'N/A (no plan in history)'
            agent_log(logging.INFO, self.state.agent_id, current_actual_step_num, 
                      f'Replaying historical step {i + 1}/{len(history_to_replay.history)} (Original goal: "{historical_goal}")')

            if not historical_step_item.agent_output or not historical_step_item.agent_output.action:
                agent_log(logging.WARNING, self.state.agent_id, current_actual_step_num, f'Historical step {i + 1} has no actions to replay. Skipping.')
                no_action_res = ActionResult(action_name="replay_skip_no_historical_actions", success=True, extracted_content="No actions in historical step data.")
                all_rerun_action_results_objects.append(no_action_res)
                current_bs_for_skip = await self.browser_context.get_state(False)
                self._make_history_item(None, current_bs_for_skip, [no_action_res], StepMetadata(step_number=current_actual_step_num, step_start_time=step_start_time_rerun, step_end_time=time.monotonic()))
                self.state.n_steps +=1 # Increment step even for skipped replay
                continue

            retry_count = 0
            step_replayed_successfully = False
            current_step_rerun_results: List[ActionResult] = []

            while retry_count <= max_retries_per_step:
                live_browser_state_for_rerun = await self.browser_context.get_state(cache_clickable_elements_hashes=True)
                actions_for_this_attempt: List[ActionModel] = []
                mapping_ok = True
                for action_idx, original_action in enumerate(historical_step_item.agent_output.action):
                    hist_elem_info = None
                    if historical_step_item.browser_state and \
                       action_idx < len(historical_step_item.browser_state.interacted_element):
                        hist_elem_info = historical_step_item.browser_state.interacted_element[action_idx]
                    
                    updated_action = await self._update_action_for_rerun(hist_elem_info, original_action, live_browser_state_for_rerun)
                    if updated_action is None: 
                        mapping_ok = False
                        agent_log(logging.ERROR, self.state.agent_id, current_actual_step_num, f"Failed to map historical action {action_idx+1} to current page. Cannot replay this action reliably.")
                        break
                    actions_for_this_attempt.append(updated_action)
                
                if not mapping_ok: 
                    current_step_rerun_results = [self.controller._create_action_result(action_name="replay_action_mapping_failed", parameters=None, success=False, error_message="Could not map historical actions to current page state.")]
                    break 

                try:
                    # Call Agent's multi_act, not controller's directly
                    current_step_rerun_results = await self.multi_act( 
                        actions_from_llm_model_instances=actions_for_this_attempt, 
                        browser_context=self.browser_context 
                    )
                    
                    if all(ar.success for ar in current_step_rerun_results):
                        step_replayed_successfully = True
                        break 
                    else:
                        first_fail_msg = next((ar.error for ar in current_step_rerun_results if not ar.success and ar.error), "Unknown failure in replayed step.")
                        raise RuntimeError(f"Replayed step had a failed action: {first_fail_msg}")

                except Exception as e_rerun:
                    retry_count += 1
                    agent_log(logging.WARNING, self.state.agent_id, current_actual_step_num, 
                              f"Replay of hist-step {i+1} failed (attempt {retry_count}/{max_retries_per_step+1}): {e_rerun}")
                    if retry_count > max_retries_per_step:
                        final_err_msg = f'Hist-step {i+1} failed re-execution after {max_retries_per_step+1} attempts: {str(e_rerun)}'
                        agent_log(logging.ERROR, self.state.agent_id, current_actual_step_num, final_err_msg)
                        current_step_rerun_results = [self.controller._create_action_result(action_name="replay_step_max_retries", parameters=None, success=False, error_message=final_err_msg)]
                        if not skip_failures:
                            all_rerun_action_results_objects.extend(current_step_rerun_results)
                            self._make_history_item(historical_step_item.agent_output, live_browser_state_for_rerun, current_step_rerun_results, StepMetadata(step_number=current_actual_step_num, step_start_time=step_start_time_rerun, step_end_time=time.monotonic()))
                            self.state.n_steps +=1 
                            raise RuntimeError(final_err_msg) from e_rerun
                        break 
                    else:
                        await asyncio.sleep(delay_between_actions * retry_count) 
            
            all_rerun_action_results_objects.extend(current_step_rerun_results)
            self._make_history_item(historical_step_item.agent_output, live_browser_state_for_rerun, current_step_rerun_results, StepMetadata(step_number=current_actual_step_num, step_start_time=step_start_time_rerun, step_end_time=time.monotonic()))
            self.state.n_steps +=1 

            if not step_replayed_successfully and not skip_failures:
                agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps-1, f"Halting history rerun due to critical failure at replayed hist-step {i+1}.")
                break
            if not step_replayed_successfully and skip_failures:
                agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps-1, f"Skipping failed hist-step {i+1} and continuing rerun.")


        agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, f"Finished replaying history. Total new steps taken in current run: {self.state.n_steps - initial_step_count_for_rerun}")
        return all_rerun_action_results_objects


    async def _update_action_for_rerun( # Added async as it's called by async rerun_history
        self,
        historical_element_targeted: Optional[DOMHistoryElement], 
        original_action: ActionModel,                             
        current_browser_state: BrowserState,                      
    ) -> Optional[ActionModel]:
        action_dump = original_action.model_dump(exclude_unset=True)
        action_name = next(iter(action_dump))
        action_params = action_dump[action_name]

        if not (isinstance(action_params, dict) and 'index' in action_params):
            return original_action 
        if not historical_element_targeted:
            agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, f"No historical element info for action '{action_name}', using original index {action_params.get('index')}. May be fragile.")
            return original_action

        from browser_use.dom.history_tree_processor.service import HistoryTreeProcessor # Local import
        current_element_node_match = HistoryTreeProcessor.find_history_element_in_tree(
            historical_element_targeted, current_browser_state.element_tree 
        )

        if current_element_node_match and current_element_node_match.highlight_index is not None:
            original_index = action_params['index']
            new_index = current_element_node_match.highlight_index
            if original_index != new_index:
                agent_log(logging.INFO, self.state.agent_id, self.state.n_steps,
                          f"Remapped element for action '{action_name}'. Old index: {original_index} -> New index: {new_index}. "
                          f"(Element: {current_element_node_match.tag_name}, XPath suffix: ...{historical_element_targeted.xpath[-30:]})")
            
            updated_params_dict = {**action_params, "index": new_index}
            
            action_info_from_registry = self.controller.registry.actions.get(action_name) # Corrected: self.controller.registry.actions
            if action_info_from_registry and action_info_from_registry.param_model:
                try:
                    validated_updated_params_obj = action_info_from_registry.param_model(**updated_params_dict)
                    return self.ActionModel(**{action_name: validated_updated_params_obj})
                except ValidationError as ve:
                    agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps, f"ValidationError creating updated action '{action_name}' for rerun with new index: {ve}")
                    return None 
            else:
                agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps, f"Could not find param_model for action '{action_name}' in registry during rerun update.")
                return None
        else: 
            agent_log(logging.WARNING, self.state.agent_id, self.state.n_steps,
                      f"Could not find a reliable match in current DOM for historical element targeted by action '{action_name}' (old index {action_params.get('index')}, XPath suffix: ...{historical_element_targeted.xpath[-30:]}). Action may fail or target incorrectly.")
            return None 

    def pause(self) -> None: 
        agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "Agent pause sequence initiated by callback.")
        import sys # Import sys for stderr
        print('\n\nâ¸ï¸  Agent pause requested. Execution will halt gracefully. Browser remains open.', file=sys.stderr)
        self.state.status = AgentStatus.PAUSED 

    def resume(self) -> None: 
        agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "Agent resume sequence initiated by callback.")
        import sys # Import sys for stderr
        print('----------------------------------------------------------------------', file=sys.stderr)
        print('â–¶ï¸  Resuming agent execution...\n', file=sys.stderr)
        self.state.status = AgentStatus.RUNNING
        
    def stop(self) -> None: 
        agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, f"Agent stop sequence initiated. Current status: {self.state.status.value}")
        if self.state.status not in [AgentStatus.STOPPED, AgentStatus.FAILED, AgentStatus.COMPLETED]:
            self.state.status = AgentStatus.STOPPED
            self.state.accumulated_output = self.state.accumulated_output or "Agent run explicitly stopped by stop() call."
        else:
            agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "Agent already in a terminal state or stop already requested.")


    async def _run_planner(self, current_browser_state_for_planner: BrowserState) -> Optional[ReflectionPlannerOutput]: 
        if not self.planner_llm: # Corrected from self.settings.planner_llm
            agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, "Skipping planner: no planner_llm configured on agent instance.")
            return None

        agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "Running reflector/planner...")
        
        if not getattr(self.planner_llm, '_verified_api_keys', False): # Corrected from self.settings.planner_llm
             if not await self._verify_llm_connection(self.planner_llm, 'Planner LLM (_run_planner)'): # Corrected
                agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps, "Planner LLM connection verification failed. Skipping planning.")
                return None

        history_slice = self.state.history.history[-HISTORY_FOR_PLANNER:]
        recent_history_str_parts = []
        for h_item in reversed(history_slice):
            outcome_parts = ["  Outcomes:"]
            if h_item.action_results: 
                for i, res in enumerate(h_item.action_results): 
                    outcome_parts.append(f"    Action {i+1} ({res.action_name}): {'Success' if res.success else 'Failure'}" + (f" -> Err: {res.error[:70]}..." if res.error else (f" -> Out: {res.extracted_content[:70]}..." if res.extracted_content else "")))
            else: outcome_parts.append("    No actions reported for this historical step.")
            planned_goal = h_item.agent_output.current_state.next_goal if h_item.agent_output else "N/A"
            recent_history_str_parts.append(f"Step {h_item.step}:\n  Planned Goal: '{planned_goal}'\n" + "\n".join(outcome_parts))
        recent_history_formatted = "\n---\n".join(recent_history_str_parts) if recent_history_str_parts else "No recent execution history available."

        planner_prompt_obj = PlannerPrompt() 
        
        planner_context_args = {
            "task": self.state.task.get('instructions', 'N/A'), 
            "previous_plan": self.state.current_goal or "Define initial strategy.", 
            "recent_history": recent_history_formatted,
            "current_url": current_browser_state_for_planner.url or "Unknown",
            "error_context_for_planner": self.state.last_error or "No specific error in the last step." # Added error_context_for_planner
        }
        planner_messages = planner_prompt_obj.get_prompt_messages(
            use_vision=self.settings.use_vision_for_planner,
            screenshot_base64=current_browser_state_for_planner.screenshot,
            **planner_context_args
        )
        
        final_planner_messages = convert_input_messages(planner_messages, self.planner_model_name)

        try:
            agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, "Invoking planner LLM...")
            raw_response = await self.planner_llm.ainvoke(final_planner_messages) # Corrected from self.settings.planner_llm
            response_content = str(raw_response.content)

            if self.planner_model_name and ("deepseek-r1" in self.planner_model_name.lower() or "deepseek-reasoner" in self.planner_model_name.lower()):
                response_content = self._remove_think_tags(response_content) 

            try:
                planner_json_dict = extract_json_from_model_output(response_content) 
                parsed_output = ReflectionPlannerOutput.model_validate(planner_json_dict)
                agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, f"Planner Analysis Received. New Goal: {parsed_output.next_goal[:100]}... Summary: {parsed_output.memory_summary[:100]}...")
                return parsed_output
            except (LLMOutputParsingError, ValidationError) as e_parse:
                agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps, f"Failed to parse planner output: {e_parse}. Raw: {response_content[:200]}...")
                return None

        except Exception as e:
            agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps, f"Failed to invoke planner LLM: {e}", exc_info=True)
            return None 

    @property
    def message_manager(self) -> MessageManager: 
        return self._message_manager

    async def close(self): 
        agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "Agent close() initiated.")
        try:
            if hasattr(self, 'browser_context') and self.browser_context and not self.injected_browser_context: 
                if hasattr(self.browser_context, 'config') and not self.browser_context.config.keep_alive:
                    agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, "Closing owned BrowserContext.")
                    await self.browser_context.close()
                elif not hasattr(self.browser_context, 'config'): 
                    agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, "Closing non-injected BrowserContext (no keep_alive config found).")
                    await self.browser_context.close()


            if hasattr(self, 'browser') and self.browser and not self.injected_browser:
                if hasattr(self.browser, 'config') and not self.browser.config.keep_alive:
                    agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, "Closing owned Browser.")
                    await self.browser.close()
                elif not hasattr(self.browser, 'config'):
                    agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, "Closing non-injected Browser (no keep_alive config found).")
                    await self.browser.close()
            
            if hasattr(self, 'memory') and self.memory and hasattr(self.memory, 'close'): 
                agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, "Closing memory resources.")
                # await self.memory.close() # Assuming memory.close() is async if it exists

            gc.collect()
            agent_log(logging.INFO, self.state.agent_id, self.state.n_steps, "Agent resources cleanup attempt finished.")

        except Exception as e:
            logger.error(f'Error during agent resource cleanup: {e}', exc_info=True)


    async def _update_action_models_for_page(self, page_obj: Optional[Any]) -> None: 
        try:
            self.ActionModel = self.controller.registry.create_action_model(page=page_obj)
            self.AgentOutput = AgentOutput.type_with_custom_actions(self.ActionModel)

            self.DoneActionModel = self.controller.registry.create_action_model(include_actions=['done'], page=page_obj)
            self.DoneAgentOutput = AgentOutput.type_with_custom_actions(self.DoneActionModel)
            agent_log(logging.DEBUG, self.state.agent_id, self.state.n_steps, 
                      f"Action schemas updated for page context. Current ActionModel: {self.ActionModel.__name__} with fields {list(self.ActionModel.model_fields.keys())}")
        except Exception as e:
            agent_log(logging.ERROR, self.state.agent_id, self.state.n_steps,
                      f"Failed to update action models for page context. Using previous schemas. Error: {e}", exc_info=True)