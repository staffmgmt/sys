# filename: newp/browser_use/agent/message_manager/views.py
from __future__ import annotations

import uuid
from typing import TYPE_CHECKING, Any, List, Optional, Dict # Added List, Optional
from warnings import filterwarnings

from langchain_core._api import LangChainBetaWarning
from langchain_core.load import dumpd, load
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, ToolMessage
from pydantic import BaseModel, ConfigDict, Field, model_serializer, model_validator

filterwarnings('ignore', category=LangChainBetaWarning) # Apply filter at module level

if TYPE_CHECKING:
	from browser_use.agent.views import AgentOutput # Used in MessageHistory


class MessageMetadata(BaseModel):
	"""Metadata associated with a message, like token count and type."""
	tokens: int = Field(default=0, description="Number of tokens in the message.")
	message_type: Optional[str] = Field(None, description="A specific type or category for the message, e.g., 'system_init', 'action_results'. Defaults to BaseMessage.type if None.")


class ManagedMessage(BaseModel):
	"""A Langchain BaseMessage bundled with its metadata."""
	message: BaseMessage = Field(description="The core Langchain message object.")
	metadata: MessageMetadata = Field(default_factory=MessageMetadata, description="Metadata for this message.")

	model_config = ConfigDict(arbitrary_types_allowed=True)

	# Custom serializer to handle Langchain's BaseMessage object within the 'message' field.
	# https://github.com/pydantic/pydantic/discussions/7558
	@model_serializer(mode='wrap')
	def to_json(self, original_dump_method: Callable[..., Dict[str, Any]]) -> Dict[str, Any]:
		"""
		Serializes the ManagedMessage to a JSON-compatible dictionary.
		Uses Langchain's `dumpd` for the 'message' field to ensure proper serialization
		of BaseMessage and its subclasses.
		"""
		data = original_dump_method(self)
		# Override the 'message' field's serialization to use Langchain's dumpd function.
		data['message'] = dumpd(self.message)
		return data

	# Custom validator to reconstruct Langchain's BaseMessage object during parsing.
	@model_validator(mode='before')
	@classmethod
	def validate_message_field(
		cls,
		values: Any,
		# These args are part of model_validator signature, but not always used explicitly
		# strict: Optional[bool] = None, 
		# from_attributes: Optional[bool] = None,
		# context: Optional[Any] = None,
	) -> Any:
		"""
		Validates and parses the input data before model creation.
		If 'message' field is a dictionary (likely from JSON), it uses Langchain's `load`
		to reconstruct the appropriate BaseMessage subclass.
		"""
		if isinstance(values, dict) and 'message' in values:
			message_data = values['message']
			if isinstance(message_data, dict): # It should be a dict if coming from serialized JSON
				# Reconstruct the BaseMessage object using Langchain's load function
				filterwarnings('ignore', category=LangChainBetaWarning) # Suppress warning during load
				values['message'] = load(message_data)
			elif not isinstance(message_data, BaseMessage):
			    # If it's not a dict and not already a BaseMessage, this might be an issue.
			    # Pydantic might handle basic types, but for complex ones, load is needed.
			    # Depending on strictness, could raise error or log warning.
			    pass # Let Pydantic try, or add more specific error handling.
		return values


class MessageHistory(BaseModel):
	"""Manages a history of messages, each with associated metadata including token counts."""
	messages: List[ManagedMessage] = Field(default_factory=list, description="The list of managed messages in chronological order.")
	current_tokens: int = Field(default=0, description="The total number of tokens currently in the history.")

	model_config = ConfigDict(arbitrary_types_allowed=True)

	def add_message(self, message: BaseMessage, metadata: MessageMetadata, position: Optional[int] = None) -> None:
		"""Adds a message with its metadata to the history, optionally at a specific position."""
		managed_msg = ManagedMessage(message=message, metadata=metadata)
		if position is None:
			self.messages.append(managed_msg)
		else:
			# Ensure position is valid
			pos = max(0, min(position, len(self.messages)))
			self.messages.insert(pos, managed_msg)
		self.current_tokens += metadata.tokens

	def add_model_output(self, output: AgentOutput) -> None:
		"""
		Adds an AIMessage representing the agent's output (AgentOutput model) to history.
		This specific method formats the AgentOutput as if it were a tool call made by the LLM.
		"""
		# Create a tool call structure for the AgentOutput
		tool_calls = [
			{
				'name': 'AgentOutput', # Treating AgentOutput itself as the "tool" name
				'args': output.model_dump(mode='json', exclude_unset=True), # Parameters are the AgentOutput
				'id': str(uuid.uuid4()), # Generate a unique ID for this pseudo tool call
				'type': 'tool_call', # Langchain specific type
			}
		]

		# Create an AIMessage with this tool_call structure
		# Content can be empty or a brief note if needed.
		ai_msg_with_tool_call = AIMessage(
			content="", # Or "Agent planned the following output/actions:"
			tool_calls=tool_calls,
		)
		# Estimate tokens for this complex AIMessage (placeholder, real tokenization needed for accuracy)
		self.add_message(ai_msg_with_tool_call, MessageMetadata(tokens=100, message_type='ai_tool_call_agent_output'))

		# Per Langchain convention, a tool call is often followed by a ToolMessage response.
		# Here, since AgentOutput isn't a "real" tool that returns something to the LLM *in the same turn*,
		# an empty ToolMessage can signify acknowledgement or completion of this "AgentOutput tool".
		# This seems to be the pattern the original user-provided agent/service.py implies by calling this.
		if tool_calls[0].get('id'):
			tool_response_message = ToolMessage(content="[AgentOutput processed]", tool_call_id=tool_calls[0]['id'])
			# Estimate tokens for this simple ToolMessage
			self.add_message(tool_response_message, MessageMetadata(tokens=10, message_type='tool_response_agent_output'))


	def get_messages(self) -> List[BaseMessage]:
		"""Returns all Langchain BaseMessage objects from the history."""
		return [m.message for m in self.messages]

	def get_total_tokens(self) -> int:
		"""Returns the total number of tokens currently estimated in the history."""
		return self.current_tokens

	def remove_oldest_message(self) -> Optional[ManagedMessage]:
		"""
		Removes the oldest non-SystemMessage from the history and returns it.
		Updates the total token count. Returns None if no suitable message is found.
		"""
		for i, managed_msg in enumerate(self.messages):
			if not isinstance(managed_msg.message, SystemMessage):
				self.current_tokens -= managed_msg.metadata.tokens
				return self.messages.pop(i)
		return None

	def remove_last_state_message(self) -> Optional[ManagedMessage]:
		"""
		Removes the last message if it's a HumanMessage, intended for specific scenarios
		where a temporary state observation (as HumanMessage) needs to be cleaned up
		before the next LLM call. Assumes history has at least 3 messages for this to be safe.
		(This method's utility depends on how MessageManager service uses it).
		"""
		if len(self.messages) > 2 and isinstance(self.messages[-1].message, HumanMessage):
			removed_msg = self.messages.pop()
			self.current_tokens -= removed_msg.metadata.tokens
			return removed_msg
		return None


class MessageManagerState(BaseModel):
	"""Holds the state for the MessageManager service, including history and tool ID counter."""
	history: MessageHistory = Field(default_factory=MessageHistory)
	tool_id: int = Field(default=1, description="Counter for generating unique tool_call_ids if not provided by LLM.")

	model_config = ConfigDict(arbitrary_types_allowed=True)