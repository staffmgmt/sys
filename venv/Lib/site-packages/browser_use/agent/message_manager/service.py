# filename: newp/browser_use/agent/message_manager/service.py
from __future__ import annotations

import json
import logging
from typing import Any, Dict, List, Optional, Union # Added List

from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
from pydantic import BaseModel, Field # Field for MessageManagerSettings

# Imports from within the browser_use package
from browser_use.agent.message_manager.views import ManagedMessage, MessageMetadata, MessageManagerState
from browser_use.agent.prompts import AgentMessagePrompt # For formatting current browser state
from browser_use.agent.views import (
    AgentHistory, 
    ActionResult, 
    AgentOutput, # For _format_assistant_output
    AgentStatus, # For contextual prompting
    AgentBrain # For formatting AgentOutput
)
from browser_use.browser.views import BrowserState
# ActionModel is the dynamic one created by the registry, used in AgentOutput.
# The type hint in _format_actions_for_history will be 'ActionModel'.
from browser_use.controller.registry.views import ActionModel

from browser_use.utils import time_execution_sync

logger = logging.getLogger(__name__)


class MessageManagerSettings(BaseModel):
    """Settings for the MessageManager."""
    max_input_tokens: int = Field(128000, description="Maximum tokens allowed for the LLM input prompt.")
    estimated_characters_per_token: int = Field(4, ge=1, description="Estimated characters per token for rough text token counting.")
    image_tokens: int = Field(800, ge=0, description="Estimated token cost for including an image in the prompt.") 
    include_attributes: List[str] = Field(
        default_factory=lambda: [
            'title', 'type', 'name', 'role', 'tabindex', 
            'aria-label', 'placeholder', 'value', 'alt', 'aria-expanded',
        ],
        description="HTML attributes to include in DOM representation."
    )
    message_context: Optional[str] = Field(None, description="Static additional context string to include in messages.")
    sensitive_data: Optional[Dict[str, str]] = Field(None, description="Dictionary of sensitive data placeholders and their actual values for filtering.")
    available_file_paths: Optional[List[str]] = Field(None, description="List of file paths accessible to the agent, for context.")
    recent_message_window_priority: int = Field(5, ge=1, description="Number of recent conversational turns (AI + Human/Tool) to prioritize keeping during truncation.")


class MessageManager:
    def __init__(
        self,
        task_description: str,
        system_prompt_message: SystemMessage, # Actual SystemMessage object
        settings: MessageManagerSettings = MessageManagerSettings(),
        state: MessageManagerState = MessageManagerState(), 
    ):
        self.task_description = task_description
        self.settings = settings
        self.state = state 
        self.system_prompt_message = system_prompt_message

        if not self.state.history.messages:
            self._initialize_message_history()

    def _initialize_message_history(self) -> None:
        """Initializes the message history with system prompt and static contextual messages."""
        logger.debug("Initializing new message history.")
        self.state.history.messages.clear() 
        self.state.history.current_tokens = 0
        self.state.tool_id = 1 # Start tool_id from 1

        self._add_message_with_tokens(self.system_prompt_message, message_type='system_init')

        if self.settings.message_context:
            context_msg_obj = HumanMessage(content=f"General Context for Task: {self.settings.message_context}")
            self._add_message_with_tokens(context_msg_obj, message_type='static_context_init')
        
        if self.settings.sensitive_data:
            sensitive_info = (
                f"Note: Sensitive data placeholders like <secret>placeholder_name</secret> might be used. "
                f"Recognized placeholders: {list(self.settings.sensitive_data.keys())}"
            )
            self._add_message_with_tokens(HumanMessage(content=sensitive_info), message_type='sensitive_data_info_init')

        if self.settings.available_file_paths:
            filepaths_info = f"Accessible file paths for context (if tools are used): {', '.join(self.settings.available_file_paths)}"
            self._add_message_with_tokens(HumanMessage(content=filepaths_info), message_type='filepath_info_init')
        
        logger.info(f"Message history initialized. Current tokens: {self.state.history.current_tokens}")


    def _add_message_with_tokens(
        self, message: BaseMessage, position: Optional[int] = None, message_type: Optional[str] = None
    ) -> None:
        """Adds a message to history with its token count, optionally at a specific position."""
        if self.settings.sensitive_data:
            message = self._filter_sensitive_data(message)

        token_count = self._count_tokens(message)
        # Use message.type (e.g. 'human', 'ai', 'system') as default metadata type if not provided
        metadata = MessageMetadata(tokens=token_count, message_type=message_type or message.type) 
        
        self.state.history.add_message(message, metadata, position)
        logger.debug(f"Added {message.type} message (metadata_type: {metadata.message_type}, tokens: {token_count}). Total tokens: {self.state.history.current_tokens}")

    @time_execution_sync('--filter_sensitive_data')
    def _filter_sensitive_data(self, message: BaseMessage) -> BaseMessage:
        """Filters sensitive data from message content based on settings."""
        if not self.settings.sensitive_data:
            return message
        
        original_content = message.content
        new_content = original_content
        changed = False

        def replace_sensitive_in_string(value: str) -> str:
            nonlocal changed # To track if any replacement occurred
            original_value_for_replacement = value
            if not self.settings.sensitive_data: return value # Should not happen due to outer check
            for placeholder_key, actual_value in self.settings.sensitive_data.items():
                if actual_value and actual_value in value: 
                    value = value.replace(actual_value, f"<secret>{placeholder_key}</secret>")
            if value != original_value_for_replacement:
                changed = True
            return value

        if isinstance(original_content, str):
            new_content = replace_sensitive_in_string(original_content)
        elif isinstance(original_content, list): # For multimodal messages
            processed_list_content = []
            list_item_changed_flag = False # Track changes within the list processing
            for item in original_content:
                if isinstance(item, dict) and item.get("type") == "text" and isinstance(item.get("text"), str):
                    original_text = item["text"]
                    modified_text = replace_sensitive_in_string(original_text) # 'changed' (nonlocal) will be set if this call makes a change
                    if original_text != modified_text:
                        list_item_changed_flag = True
                    processed_list_content.append({**item, "text": modified_text})
                else:
                    processed_list_content.append(item)
            if list_item_changed_flag: # If any item in the list was modified
                new_content = processed_list_content
                # 'changed' (nonlocal) would have been set by replace_sensitive_in_string if any string was modified
            else: # No item's text content changed, so the list itself is unchanged effectively
                pass # 'changed' remains whatever it was before list processing (likely False if only list was present)

        if changed: # If any replacement occurred in string or list items
            try:
                # Create a new message instance of the same type with the modified content
                # Special handling for AIMessage with tool_calls or other complex attributes
                if isinstance(message, AIMessage) and (message.tool_calls or message.invalid_tool_calls):
                     return AIMessage(
                         content=new_content, 
                         tool_calls=message.tool_calls, 
                         invalid_tool_calls=message.invalid_tool_calls,
                         id=message.id if hasattr(message, 'id') else None,
                         additional_kwargs=message.additional_kwargs
                    )
                # For other BaseMessage types
                return type(message)(content=new_content, id=message.id if hasattr(message, 'id') else None, additional_kwargs=getattr(message, 'additional_kwargs', {}))
            except Exception as e:
                logger.warning(f"Could not create new message instance after filtering sensitive data, returning original. Type: {type(message)}, Error: {e}")
                return message 
        return message

    def _count_tokens(self, message: BaseMessage) -> int:
        """
        Estimates token count for a message.
        NOTE: This is a very rough estimation. For accurate token counting,
        use a tokenizer specific to the LLM being used (e.g., tiktoken for OpenAI).
        """
        current_tokens = 0
        if isinstance(message.content, str):
            current_tokens += len(message.content) // self.settings.estimated_characters_per_token
        elif isinstance(message.content, list): # For multimodal messages
            for item in message.content:
                if isinstance(item, dict):
                    if item.get("type") == "text" and isinstance(item.get("text"), str):
                        current_tokens += len(item["text"]) // self.settings.estimated_characters_per_token
                    elif item.get("type") == "image_url":
                        current_tokens += self.settings.image_tokens
                    else: # Default token estimate for unknown dict item types in multimodal list
                        current_tokens += 20 
        
        # Handle tool_calls more robustly
        if isinstance(message, AIMessage):
            if hasattr(message, 'tool_calls') and message.tool_calls:
                for tool_call in message.tool_calls: # Iterate and sum for each tool_call
                    if isinstance(tool_call, dict): # Ensure tool_call is a dict before dumping
                        try:
                            tc_str = json.dumps(tool_call) 
                            current_tokens += len(tc_str) // self.settings.estimated_characters_per_token
                        except (TypeError, OverflowError):
                            current_tokens += 50 # Arbitrary penalty if individual serialization fails
                    else: # If tool_call is not a dict (unexpected)
                        current_tokens += 20 # Small penalty for unexpected structure
            
            if hasattr(message, 'invalid_tool_calls') and message.invalid_tool_calls:
                for invalid_tc in message.invalid_tool_calls: # Iterate and sum for each invalid_tool_call
                    if isinstance(invalid_tc, dict):
                        try:
                            invalid_tc_str = json.dumps(invalid_tc)
                            current_tokens += len(invalid_tc_str) // self.settings.estimated_characters_per_token
                        except (TypeError, OverflowError):
                            current_tokens += 50
                    else:
                         current_tokens += 20


        if isinstance(message, ToolMessage) and hasattr(message, 'tool_call_id'):
            current_tokens += len(message.tool_call_id) // self.settings.estimated_characters_per_token 
            current_tokens += 2 

        return max(1, current_tokens)
    def _format_agentbrain_for_history(self, agent_brain: AgentBrain) -> str:
        """Formats AgentBrain for inclusion in AI message history content."""
        return (
            f"Thought Process (Assessment of Prior Actions, Task Log, Next Goal):\n"
            f"  Assessment: {agent_brain.prior_action_assessment}\n"
            f"  Task Log: {agent_brain.task_log}\n"
            f"  Next Goal (for chosen actions): {agent_brain.next_goal}"
        )

    def _format_actions_for_history(self, actions: List[ActionModel]) -> str:
        """Formats a list of ActionModel instances for inclusion in AI message history content."""
        action_strs = []
        if not actions:
            return "Planned Actions: None"
        for i, action_model_instance in enumerate(actions):
            # ActionModel is a discriminated union. Dump it to get the actual action.
            action_dump = action_model_instance.model_dump(exclude_unset=True)
            if not action_dump: # Handle case where action_model_instance might be empty
                action_strs.append(f"  {i+1}. Malformed or Empty Action")
                logger.warning(f"Encountered an empty action_dump for action model instance: {action_model_instance} in _format_actions_for_history.")
                continue
            action_name = next(iter(action_dump)) # Get the first (and only) key, which is the action name
            action_params = action_dump[action_name]
            try:
                # Serialize params to JSON string if they exist, otherwise show empty.
                params_str = json.dumps(action_params, ensure_ascii=False) if action_params else "{}"
            except TypeError: 
                params_str = str(action_params) # Fallback for non-serializable params
            action_strs.append(f"  {i+1}. {action_name}({params_str})")
        return "Planned Actions:\n" + "\n".join(action_strs)

    def _format_actionresults_for_history(self, action_results: List[ActionResult]) -> str:
        """Formats ActionResult list for inclusion as Tool/Human message history content."""
        lines = []
        if not action_results:
            return "No actions were executed or no results reported for the previous step's plan."
        
        for i, res in enumerate(action_results):
            status = "SUCCESS" if res.success else "FAILURE"
            line = f"Outcome of Action {i+1} ({res.action_name}): Status: {status}"
            if res.error:
                line += f" | Error: {res.error[:300]}" # Truncate long errors
            elif res.extracted_content: # Prioritize extracted_content if available
                line += f" | Output: {res.extracted_content[:300]}" # Truncate long content
            elif res.raw_tool_outputs and isinstance(res.raw_tool_outputs.get('message'), str): # Fallback to raw message
                 line += f" | Message: {res.raw_tool_outputs['message'][:300]}"
            elif not res.success: # For failures with no specific error message
                 line += f" | Note: Action failed without specific error message."
            else: # For successes with no specific output message
                 line += f" | Note: Action succeeded without specific output message."
            lines.append(line)
        return "\n".join(lines)

    def _history_to_messages(self, agent_history_list: List[AgentHistory]) -> List[ManagedMessage]:
        """Converts AgentHistory entries into ManagedMessages for prompt context."""
        history_messages: List[ManagedMessage] = []

        for entry in agent_history_list:
            # 1. Assistant's output (AgentOutput: AgentBrain + planned Actions)
            if entry.agent_output:
                brain_str = self._format_agentbrain_for_history(entry.agent_output.current_state)
                actions_str = self._format_actions_for_history(entry.agent_output.action)
                # Construct content for the AI's turn
                ai_content = f"{brain_str}\n{actions_str}"
                ai_msg = AIMessage(content=ai_content)
                # The metadata_type 'assistant_output' is just for internal tracking during assembly
                self._add_message_with_tokens_to_list(ai_msg, 'assistant_output', history_messages)

            # 2. Tool/Action results (ActionResults)
            # Represented as a HumanMessage summarizing all tool outcomes for this step.
            if entry.action_results:
                results_summary_str = self._format_actionresults_for_history(entry.action_results)
                human_msg_content = f"--- Action Execution Outcomes ---\n{results_summary_str}\n--- End Outcomes ---"
                human_msg = HumanMessage(content=human_msg_content)
                self._add_message_with_tokens_to_list(human_msg, 'action_results', history_messages)
            elif entry.agent_output : # If there was an AI output but no action results (e.g., LLM decided 'done' or error in planning)
                # Could add a placeholder if needed, e.g., "No actions were executed in response to the above plan."
                # For now, if no action_results, no corresponding message is added, which is fine.
                pass 
        
        return history_messages

    def _add_message_with_tokens_to_list(self, message: BaseMessage, internal_category_label: str, target_list: List[ManagedMessage]):
        """Helper to add a message with its token count to a given list (for temporary assembly)."""
        if self.settings.sensitive_data:
            message = self._filter_sensitive_data(message)
        token_count = self._count_tokens(message)
        # Use the provided internal_category_label for MessageMetadata.message_type,
        # which is used by prepare_messages for sorting and retrieval.
        metadata = MessageMetadata(tokens=token_count, message_type=internal_category_label) 
        target_list.append(ManagedMessage(message=message, metadata=metadata))

    def prepare_messages(
        self,
        agent_history_for_prompt: List[AgentHistory], 
        task_instructions: str, 
        current_browser_state_for_prompt: BrowserState, 
        current_goal: Optional[str],
        current_memory_summary: Optional[str],
        agent_status: AgentStatus,
        agent_id: Optional[str] = None, 
        last_error: Optional[str] = None, 
        page_specific_actions_desc: Optional[str] = None, 
        system_prompt_override_str: Optional[str] = None, 
        max_prompt_tokens_override: Optional[int] = None,
        use_vision_for_current_state: bool = True
    ) -> List[BaseMessage]:
        """
        Prepares the list of messages for the LLM, including system prompt, task,
        contextual info, formatted history, and current observation. Performs truncation.
        """
        max_tokens = max_prompt_tokens_override or self.settings.max_input_tokens
        logger.debug(
            f"Preparing messages for LLM. Max tokens: {max_tokens}. Agent Status: {agent_status.value}.",
            extra={'agent_id': agent_id or "N/A"}
        )

        all_potential_messages: List[ManagedMessage] = [] 

        # 1. System Prompt
        current_system_prompt_obj = SystemMessage(content=system_prompt_override_str) if system_prompt_override_str else self.system_prompt_message
        self._add_message_with_tokens_to_list(current_system_prompt_obj, 'system_prompt', all_potential_messages)

        # 2. Memory Summary (High priority)
        if current_memory_summary:
            mem_summary_content = f"--- REFLECTION & MEMORY SUMMARY (Review this carefully!) ---\n{current_memory_summary}\n--- END SUMMARY ---"
            self._add_message_with_tokens_to_list(HumanMessage(content=mem_summary_content), 'memory_summary', all_potential_messages)

        # 3. Task Instructions & Current Goal (High priority)
        task_goal_content = f"Current Overall Task: ```{task_instructions}```"
        if current_goal:
            task_goal_content += f"\n\nYour Current Strategic Goal (Focus on this): ```{current_goal}```"
        else:
            task_goal_content += f"\n\nYour Current Strategic Goal: Determine the next best course of action for the overall task."
        self._add_message_with_tokens_to_list(HumanMessage(content=task_goal_content), 'task_and_goal', all_potential_messages)
        
        # 4. Error Context (If recovering, very high priority)
        if agent_status == AgentStatus.RECOVERING and last_error:
            error_context_content = (
                f"--- RECOVERY MODE: PREVIOUS STEP FAILED ---\n"
                f"Last Error Reported: ```{last_error}```\n"
                f"Analyze this error with the current browser state and history. "
                f"Devise actions to recover or find an alternative approach to achieve the Current Strategic Goal.\n"
                f"--- END ERROR CONTEXT ---"
            )
            self._add_message_with_tokens_to_list(HumanMessage(content=error_context_content), 'error_context', all_potential_messages)

        # 5. Page Specific Actions (Contextual)
        if page_specific_actions_desc:
            page_actions_content = f"--- Context-Specific Actions Available for Current Page ---\n{page_specific_actions_desc}\n--- END ACTIONS ---"
            self._add_message_with_tokens_to_list(HumanMessage(content=page_actions_content), 'page_actions_context', all_potential_messages)

        # 6. Formatted Agent History (These are the primary truncatable messages)
        formatted_history_messages: List[ManagedMessage] = self._history_to_messages(agent_history_for_prompt)

        # 7. Current Browser State Observation (Always last before LLM thinks)
        current_state_observation_msg_obj = AgentMessagePrompt(
            current_browser_state=current_browser_state_for_prompt, 
            include_attributes=self.settings.include_attributes,
        ).get_user_message(use_vision=use_vision_for_current_state)
        
        self._add_message_with_tokens_to_list(current_state_observation_msg_obj, 'current_observation', all_potential_messages)

        # --- Truncation Logic ---
        essential_prefix_messages = [m for m in all_potential_messages if m.metadata.message_type != 'current_observation']
        essential_tokens = sum(msg.metadata.tokens for msg in essential_prefix_messages)
        
        current_observation_managed_msg = next((m for m in all_potential_messages if m.metadata.message_type == 'current_observation'), None)
        current_observation_tokens = current_observation_managed_msg.metadata.tokens if current_observation_managed_msg else 0

        available_tokens_for_history = max_tokens - essential_tokens - current_observation_tokens

        truncated_formatted_history: List[ManagedMessage]
        if available_tokens_for_history <= 0:
            logger.warning(
                f"Token limit ({max_tokens}) too small even for essential prefix messages ({essential_tokens}) and current observation ({current_observation_tokens}). History will be empty or minimal.",
                extra={'agent_id': agent_id or "N/A"}
            )
            truncated_formatted_history = []
            if essential_tokens + current_observation_tokens > max_tokens:
                 logger.error(
                     f"CRITICAL: Essential prefix messages + current observation ({essential_tokens + current_observation_tokens}) exceed max_tokens ({max_tokens}). Prompt will be over limit.",
                     extra={'agent_id': agent_id or "N/A"}
                 )
        else:
            truncated_formatted_history = self._cut_messages_from_front(
                messages_to_truncate=formatted_history_messages,
                available_tokens=available_tokens_for_history,
                priority_window=self.settings.recent_message_window_priority 
            )

        # --- Construct Final Message List for LLM ---
        final_prompt_messages_managed: List[ManagedMessage] = []
        
        def find_and_add_from_prefix(msg_type_internal: str):
            found_msg = next((m for m in essential_prefix_messages if m.metadata.message_type == msg_type_internal), None)
            if found_msg: final_prompt_messages_managed.append(found_msg)

        find_and_add_from_prefix('system_prompt') 
        find_and_add_from_prefix('memory_summary')
        find_and_add_from_prefix('task_and_goal')
        find_and_add_from_prefix('error_context')
        find_and_add_from_prefix('page_actions_context')
        
        final_prompt_messages_managed.extend(truncated_formatted_history)
        
        if current_observation_managed_msg:
            final_prompt_messages_managed.append(current_observation_managed_msg)
        else: 
             logger.error("Critical: Current observation message was lost during prompt assembly (should not happen).", extra={'agent_id': agent_id or "N/A"})

        final_token_count = sum(msg.metadata.tokens for msg in final_prompt_messages_managed)

        if final_token_count > max_tokens:
            logger.warning(
                f"Final token count ({final_token_count}) exceeds max_tokens ({max_tokens}) after standard truncation! "
                "Applying emergency truncation to preserve essential context.",
                extra={'agent_id': agent_id or "N/A"}
            )
            
            system_msg = next((m for m in final_prompt_messages_managed if m.metadata.message_type == 'system_prompt'), None)
            memory_msg = next((m for m in final_prompt_messages_managed if m.metadata.message_type == 'memory_summary'), None)
            current_obs_msg_for_emergency = next((m for m in final_prompt_messages_managed if m.metadata.message_type == 'current_observation'), None)
            error_context_msg = next((m for m in final_prompt_messages_managed if m.metadata.message_type == 'error_context'), None)

            critical_msgs_candidates = []
            if system_msg: critical_msgs_candidates.append(system_msg)

            if agent_status == AgentStatus.RECOVERING and error_context_msg:
                critical_msgs_candidates.append(error_context_msg)
            elif memory_msg: 
                critical_msgs_candidates.append(memory_msg)
            
            if current_obs_msg_for_emergency: critical_msgs_candidates.append(current_obs_msg_for_emergency)

            seen_msg_ids = set()
            critical_msgs = []
            for msg_candidate in critical_msgs_candidates:
                if id(msg_candidate) not in seen_msg_ids:
                    critical_msgs.append(msg_candidate)
                    seen_msg_ids.add(id(msg_candidate))
            
            critical_msgs_tokens = sum(m.metadata.tokens for m in critical_msgs)
            if critical_msgs_tokens <= max_tokens:
                logger.warning(
                    f"Emergency truncation applied - returning {len(critical_msgs)} critical messages. Tokens: {critical_msgs_tokens}",
                    extra={'agent_id': agent_id or "N/A"}
                )
                final_prompt_messages_managed = critical_msgs
            elif agent_status == AgentStatus.RECOVERING and error_context_msg and system_msg and current_obs_msg_for_emergency:
                # Specific fallback for recovery: try system + error + observation if memory was too much
                recovery_critical_set = [m for m in [system_msg, error_context_msg, current_obs_msg_for_emergency] if m is not None]
                recovery_critical_tokens = sum(m.metadata.tokens for m in recovery_critical_set)
                if recovery_critical_tokens <= max_tokens:
                    logger.warning(
                        f"Emergency truncation (recovery mode, memory omitted): Using {len(recovery_critical_set)} messages (System, Error, Current Observation). Tokens: {recovery_critical_tokens}",
                        extra={'agent_id': agent_id or "N/A"}
                    )
                    final_prompt_messages_managed = recovery_critical_set
                else:
                    logger.error(
                        f"CRITICAL: Emergency truncation failed even in recovery mode. System + Error + Obs ({recovery_critical_tokens} tokens) exceed max_tokens ({max_tokens}).",
                        extra={'agent_id': agent_id or "N/A"}
                    )
                    final_prompt_messages_managed = recovery_critical_set 
            else:
                logger.error(
                    f"CRITICAL: Emergency truncation failed. Even the most critical messages ({critical_msgs_tokens} tokens) exceed max_tokens ({max_tokens}). Prompt may be unusable. Returning as is (potentially oversized).",
                    extra={'agent_id': agent_id or "N/A"}
                )
                if critical_msgs_tokens > max_tokens : # Ensure we use the (oversized) critical set if it's what we determined is most critical
                     final_prompt_messages_managed = critical_msgs

            final_token_count = sum(m.metadata.tokens for m in final_prompt_messages_managed)

        logger.info(
            f"Final prepared prompt: {len(final_prompt_messages_managed)} messages. Estimated tokens: {final_token_count}/{max_tokens}",
            extra={'agent_id': agent_id or "N/A"}
        )
        if final_token_count > max_tokens and final_prompt_messages_managed: 
            logger.error(
                f"CRITICAL OVERLIMIT: Final token count ({final_token_count}) STILL exceeds max_tokens ({max_tokens}) after any truncation attempts!",
                extra={'agent_id': agent_id or "N/A"}
            )

        return [m.message for m in final_prompt_messages_managed]


    def _cut_messages_from_front(
        self,
        messages_to_truncate: List[ManagedMessage],
        available_tokens: int,
        priority_window: int 
    ) -> List[ManagedMessage]:
        """
        Truncates messages by removing from the OLDEST first, while trying to preserve
        a window of the most RECENT messages/turns if possible.
        Assumes messages_to_truncate is ordered oldest to newest.
        """
        if available_tokens <= 0 or not messages_to_truncate:
            return []

        current_tokens_needed = sum(msg.metadata.tokens for msg in messages_to_truncate)
        if current_tokens_needed <= available_tokens:
            return messages_to_truncate 

        logger.info(
            f"History truncation needed. Tokens for history ({current_tokens_needed}) > available for history ({available_tokens}). "
            f"Attempting to keep last ~{priority_window} turns (approx {priority_window*2} messages)."
        )
        
        # Each "turn" is roughly 2 messages (AI + Human/Tool).
        # `priority_window` refers to turns, so roughly `priority_window * 2` messages.
        # This depends on how _history_to_messages structures ManagedMessage objects.
        # If one ManagedMessage = one significant event (AI output OR Tool results summary), then priority_window directly.
        # Current _history_to_messages: 1 AI output (AIMessage), 1 Tool Results (HumanMessage) = 2 ManagedMessages per step.
        # So, recent_message_window_priority (turns) * 2 = number of messages to prioritize.
        num_messages_to_prioritize = priority_window * 2 
        
        if num_messages_to_prioritize >= len(messages_to_truncate):
            # Priority window covers all or more than available history, truncate from oldest of these.
            kept_messages = list(messages_to_truncate)
            current_total_tokens = sum(m.metadata.tokens for m in kept_messages)
            while current_total_tokens > available_tokens and kept_messages:
                removed_msg = kept_messages.pop(0) 
                current_total_tokens -= removed_msg.metadata.tokens
            final_tokens = sum(m.metadata.tokens for m in kept_messages)
            logger.info(f"Truncation (priority window covered all): Kept {len(kept_messages)} history messages. Final history tokens: {final_tokens}/{available_tokens}")
            return kept_messages

        # Separate recent high-priority messages from older ones
        split_point = len(messages_to_truncate) - num_messages_to_prioritize
        
        older_messages = messages_to_truncate[:split_point]
        recent_messages_to_prioritize = messages_to_truncate[split_point:]
        
        tokens_for_recent_priority = sum(msg.metadata.tokens for msg in recent_messages_to_prioritize)

        if tokens_for_recent_priority <= available_tokens:
            kept_messages = list(recent_messages_to_prioritize)
            remaining_tokens_for_older = available_tokens - tokens_for_recent_priority
            
            for msg in reversed(older_messages): 
                if msg.metadata.tokens <= remaining_tokens_for_older:
                    kept_messages.insert(0, msg) 
                    remaining_tokens_for_older -= msg.metadata.tokens
                else:
                    break 
            final_tokens = sum(m.metadata.tokens for m in kept_messages)
            logger.info(f"Truncation kept {len(kept_messages)} history messages ({len(recent_messages_to_prioritize)} priority). Final history tokens: {final_tokens}/{available_tokens}")
            return kept_messages
        else:
            logger.warning(f"Priority window tokens ({tokens_for_recent_priority}) exceed available for history ({available_tokens}). Truncating within priority window.")
            messages_to_keep_from_priority = list(recent_messages_to_prioritize)
            current_priority_tokens = tokens_for_recent_priority
            while current_priority_tokens > available_tokens and messages_to_keep_from_priority:
                removed_msg = messages_to_keep_from_priority.pop(0) 
                current_priority_tokens -= removed_msg.metadata.tokens
            
            final_tokens = sum(m.metadata.tokens for m in messages_to_keep_from_priority)
            logger.info(f"Truncation kept {len(messages_to_keep_from_priority)} messages (from priority window). Final history tokens: {final_tokens}/{available_tokens}")
            return messages_to_keep_from_priority

    # --- Direct Message Adding Methods (used by Agent or other services) ---
    def add_new_task_message(self, new_task_description: str) -> None:
        content = f'Pivoting to a NEW primary task: """{new_task_description}""". Previous context may still be relevant, but focus on this new objective.'
        # This directly modifies self.state.history, which is MessageManagerState.history
        self._add_message_with_tokens(HumanMessage(content=content), message_type='new_task_directive')
        self.task_description = new_task_description 

    def add_system_provided_text_message(self, text: str, message_type: Optional[str] = "system_text_info") -> None:
        self._add_message_with_tokens(HumanMessage(content=text), message_type=message_type)

    def add_ai_response_message(self, agent_output: AgentOutput) -> None:
        """
        Adds the AI's structured output (AgentOutput) to history as a single AIMessage content.
        This is for logging the agent's "thought process" and planned actions.
        """
        brain_str = self._format_agentbrain_for_history(agent_output.current_state)
        actions_str = self._format_actions_for_history(agent_output.action)
        ai_content = f"{brain_str}\n{actions_str}"
        
        ai_msg = AIMessage(content=ai_content)
        self._add_message_with_tokens(ai_msg, message_type='ai_structured_output')
        
    def add_tool_result_message(self, content: str, tool_call_id: Optional[str] = None, message_type: Optional[str] = "tool_result") -> None:
        """Adds a tool execution result message to history."""
        # Ensure tool_call_id is a string. If not provided, generate one.
        # Using self.state.tool_id to generate unique IDs if one isn't passed.
        tc_id_str = str(tool_call_id if tool_call_id is not None else f"tool_{self.state.tool_id:04d}")
        if tool_call_id is None:
            self.state.tool_id += 1 # Increment if we generated one for next time
            
        msg = ToolMessage(content=str(content), tool_call_id=tc_id_str)
        self._add_message_with_tokens(msg, message_type=message_type)

    def get_current_messages_for_llm(self, agent_id: Optional[str] = None) -> List[BaseMessage]:
        """
        Gets the current list of BaseMessage objects from self.state.history (MessageManagerState).
        This is a raw getter; `prepare_messages` is for full prompt assembly with truncation.
        """
        total_tokens_debug = 0
        logger.debug(f"Retrieving {len(self.state.history.messages)} raw messages from MessageManager's internal history:", extra={'agent_id': agent_id or "N/A"})
        for i, m_managed in enumerate(self.state.history.messages):
            total_tokens_debug += m_managed.metadata.tokens
            logger.debug(
                f"  {i}: Type='{m_managed.message.type}', MetaType='{m_managed.metadata.message_type or 'N/A'}', Tokens={m_managed.metadata.tokens}. Content Preview: {str(m_managed.message.content)[:70]}...",
                extra={'agent_id': agent_id or "N/A"}
            )
        logger.debug(f"Total tokens in current raw internal history: {total_tokens_debug} (Manager's Max: {self.settings.max_input_tokens})", extra={'agent_id': agent_id or "N/A"})
        
        return [m.message for m in self.state.history.messages]