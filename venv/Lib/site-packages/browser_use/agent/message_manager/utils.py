# filename: newp/browser_use/agent/message_manager/utils.py
from __future__ import annotations

import json
import logging
import os
from typing import Any, List, Optional, Type # Changed list to List for older Python versions

from langchain_core.messages import (
	AIMessage,
	BaseMessage,
	HumanMessage,
	SystemMessage,
	ToolMessage,
)

# Import the custom exception for LLM output parsing
from browser_use.exceptions import LLMOutputParsingError

logger = logging.getLogger(__name__)


def extract_json_from_model_output(content: str) -> dict:
	"""
	Extract JSON object from model output string.
	Handles both plain JSON strings and JSON wrapped in markdown code blocks (e.g., ```json ... ```).
	"""
	try:
		# If content is wrapped in markdown code blocks, extract the JSON part
		if '```' in content:
			# Attempt to find content between ```json and ``` or just ``` and ```
			match = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", content, re.DOTALL)
			if match:
				content = match.group(1).strip()
			else:
				# Fallback if regex fails but ``` are present, could be malformed.
				# This is a basic split, might need more robustness if complex cases arise.
				parts = content.split('```')
				if len(parts) >= 2:
					content = parts[1] # Take the content after the first ```
					# Remove language identifier like 'json\n' if present at the start
					if '\n' in content:
						potential_lang, _, actual_json = content.partition('\n')
						if potential_lang.strip().lower() == 'json': # Check if it was a json specifier
							content = actual_json
						# else, assume the first line was part of the JSON or it's not 'json' block
				# If still no clear JSON, the original content might be passed to json.loads which might fail.
		
		# Parse the cleaned content
		return json.loads(content)
	except json.JSONDecodeError as e:
		logger.warning(f'Failed to parse JSON from model output. Content: "{content[:500]}..." Error: {str(e)}')
		# Raise the more specific custom exception
		raise LLMOutputParsingError(
		    message=f"Could not parse JSON response from LLM. Error: {str(e)}",
		    raw_output=content,
		    original_exception=e
		)
	except Exception as e: # Catch any other unexpected errors during extraction
	    logger.error(f"Unexpected error extracting JSON from model output: {e}. Content: \"{content[:500]}...\"", exc_info=True)
	    raise LLMOutputParsingError(
	        message=f"An unexpected error occurred while extracting JSON: {str(e)}",
	        raw_output=content,
	        original_exception=e
	    )


def convert_input_messages(input_messages: List[BaseMessage], model_name: Optional[str]) -> List[BaseMessage]:
	"""
	Convert input messages to a format that is compatible with specific LLM models if needed.
	Currently handles adjustments for DeepSeek models.
	"""
	if model_name is None:
		return input_messages
	
	# Standardize model name check (e.g., to lowercase)
	model_name_lower = model_name.lower()
	if 'deepseek-reasoner' in model_name_lower or 'deepseek-r1' in model_name_lower:
		converted_input_messages = _convert_messages_for_non_function_calling_models(input_messages)
		# DeepSeek models often prefer alternating Human/AI messages
		merged_input_messages = _merge_successive_messages(converted_input_messages, HumanMessage)
		merged_input_messages = _merge_successive_messages(merged_input_messages, AIMessage)
		return merged_input_messages
	
	return input_messages


def _convert_messages_for_non_function_calling_models(input_messages: List[BaseMessage]) -> List[BaseMessage]:
	"""
	Converts messages for models that may not natively support certain message types
	like ToolMessage or AIMessage with complex tool_calls, by simplifying them.
	"""
	output_messages: List[BaseMessage] = []
	for message in input_messages:
		if isinstance(message, (HumanMessage, SystemMessage)):
			output_messages.append(message)
		elif isinstance(message, ToolMessage):
			# Convert ToolMessage to HumanMessage, as its content is typically a response to a tool call
			output_messages.append(HumanMessage(content=f"Tool Execution Result (Tool Call ID: {message.tool_call_id}):\n{message.content}"))
		elif isinstance(message, AIMessage):
			# For AIMessages, if they have tool_calls, serialize them into the content string.
			# If they have regular content, keep it.
			if message.tool_calls:
				try:
					tool_calls_str = json.dumps(message.tool_calls)
					# Combine with existing content if any, or just use tool_calls if content is empty
					new_content = f"{message.content}\nTool Calls: {tool_calls_str}".strip() if message.content else f"Tool Calls: {tool_calls_str}"
					output_messages.append(AIMessage(content=new_content)) # Remove original tool_calls from AIMessage object itself
				except TypeError: # Handle non-serializable tool_calls gracefully
				    output_messages.append(AIMessage(content=f"{message.content}\n[Non-serializable tool_calls present]".strip() if message.content else "[Non-serializable tool_calls present]"))
			else:
				output_messages.append(message) # Keep as is if no tool_calls
		else:
			logger.warning(f"Encountered an unknown message type during conversion: {type(message)}. Passing through.")
			output_messages.append(message) # Pass through unknown types
	return output_messages


def _merge_successive_messages(messages: List[BaseMessage], class_to_merge: Type[BaseMessage]) -> List[BaseMessage]:
	"""
	Merges successive messages of the same specified type (e.g., HumanMessage, AIMessage)
	into a single message by concatenating their string content.
	Useful for models that don't allow multiple messages of the same role in a row.
	"""
	if not messages:
		return []

	merged_messages: List[BaseMessage] = []
	current_merged_content_parts: List[str] = []
	
	for i, message in enumerate(messages):
		if isinstance(message, class_to_merge):
			# Ensure content is string or can be stringified; complex content might need specific handling
			if isinstance(message.content, str):
				current_merged_content_parts.append(message.content)
			elif isinstance(message.content, list): # For multimodal messages, just take text parts for merging
			    text_parts_in_message = [item['text'] for item in message.content if isinstance(item, dict) and item.get('type') == 'text' and isinstance(item.get('text'), str)]
			    if text_parts_in_message:
			        current_merged_content_parts.append("\n".join(text_parts_in_message))
			    # Non-text parts are harder to merge meaningfully; this strategy prioritizes text.
			else: # Fallback for other content types
				try:
					current_merged_content_parts.append(str(message.content))
				except:
					logger.warning(f"Could not stringify content for merging: {type(message.content)}")


			# If it's the last message or the next one is different, finalize the merge
			if i == len(messages) - 1 or not isinstance(messages[i+1], class_to_merge):
				if current_merged_content_parts:
					final_content = "\n\n".join(current_merged_content_parts)
					# Create a new message of the same type with merged content
					# Preserve other attributes if important (e.g., message.id, message.name)
					# For simplicity, only content is merged here.
					merged_messages.append(class_to_merge(content=final_content))
					current_merged_content_parts = [] # Reset for next potential merge sequence
		else:
			# If there was a pending merge, finalize it before adding the current different message
			if current_merged_content_parts:
				final_content = "\n\n".join(current_merged_content_parts)
				merged_messages.append(class_to_merge(content=final_content))
				current_merged_content_parts = []
			
			merged_messages.append(message) # Add the non-merging message
			
	return merged_messages


# Renamed from save_conversation to align with agent service import, and refactored
def save_conversation_step_to_file(
    step_data_dict: Dict[str, Any], # Expects AgentHistory.model_dump()
    target_filepath: str,
    encoding: Optional[str] = 'utf-8'
) -> None:
	"""
	Saves the data for a single agent step (typically an AgentHistory Pydantic model dumped to a dict)
	to a JSON file.
	"""
	if not isinstance(step_data_dict, dict):
		logger.error(f"Failed to save conversation step: input data is not a dictionary (got {type(step_data_dict)}).")
		raise TypeError("step_data_dict must be a dictionary.")

	try:
		# Create parent directories if they don't exist
		if dirname := os.path.dirname(target_filepath):
			os.makedirs(dirname, exist_ok=True)

		with open(target_filepath, 'w', encoding=encoding or 'utf-8') as f:
			json.dump(step_data_dict, f, indent=2, ensure_ascii=False) # ensure_ascii=False for better unicode handling
		
		logger.debug(f"Successfully saved agent step data to {target_filepath}")

	except IOError as e:
		logger.error(f"IOError saving conversation step to {target_filepath}: {e}", exc_info=True)
		# Depending on desired behavior, could raise a custom error here.
	except TypeError as e: # json.dump might raise this for non-serializable content within the dict
		logger.error(f"TypeError during JSON serialization for {target_filepath}: {e}. Data preview: {str(step_data_dict)[:500]}...", exc_info=True)
	except Exception as e:
		logger.error(f"Unexpected error saving conversation step to {target_filepath}: {e}", exc_info=True)

# Import re for extract_json_from_model_output
import re