# --- FIXED llm_service.py ---
import asyncio
import logging
import os
from threading import Lock
from typing import Any, Dict, List, Optional, Tuple, Iterator, AsyncIterator, Union

# Langchain imports
from langchain_core.callbacks import (
    AsyncCallbackManagerForLLMRun,
    CallbackManagerForLLMRun,
)
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.outputs import ChatResult, ChatGeneration

# Google API imports
try:
    from langchain_google_genai import ChatGoogleGenerativeAI
    from google.api_core.exceptions import ResourceExhausted, ClientError, GoogleAPICallError
    GOOGLE_GENAI_INSTALLED = True
except ImportError:
    logging.warning("langchain-google-genai or google-api-core not installed. Gemini functionality will be limited.")
    GOOGLE_GENAI_INSTALLED = False
    # Define placeholders
    class ChatGoogleGenerativeAI: pass
    class ResourceExhausted(Exception): pass
    class ClientError(Exception): pass
    class GoogleAPICallError(Exception): pass

from pydantic import Field, ConfigDict, PrivateAttr

# Logger
logger = logging.getLogger("browser_use")

# Default model from environment with fallback
DEFAULT_MODEL = os.environ.get("LLM_MODEL")

class RotatingGeminiClient(BaseChatModel):
    """
    A ChatModel implementation that rotates through a list of Google Gemini API keys.
    """
    model_config = ConfigDict(extra='allow', arbitrary_types_allowed=True)

    # Required fields
    api_keys: List[str] = Field(..., description="List of Google AI API keys to rotate through.")
    
    # Optional fields with defaults from environment
    model_name: str = Field(default_factory=lambda: DEFAULT_MODEL, description="Gemini model to use")
    temperature: float = Field(default=0.7, description="Temperature for response generation")
    top_p: Optional[float] = Field(default=None, description="Top-p sampling parameter")
    top_k: Optional[int] = Field(default=None, description="Top-k sampling parameter")
    convert_system_message_to_human: bool = Field(default=True, description="Convert system messages to human format")
    request_timeout: Optional[float] = Field(default=120.0, description="Timeout for API requests in seconds")
    other_model_kwargs: Dict[str, Any] = Field(default_factory=dict, description="Additional model parameters")

    # Private attributes
    _client_instances: Dict[str, ChatGoogleGenerativeAI] = PrivateAttr(default_factory=dict)
    _current_key_index: int = PrivateAttr(0)
    _lock: Lock = PrivateAttr(default_factory=Lock)
    _verified_api_keys: bool = PrivateAttr(False)

    def __init__(self, **data: Any):
        super().__init__(**data)
        if not GOOGLE_GENAI_INSTALLED:
            raise ImportError("ChatGoogleGenerativeAI could not be imported. Please install langchain-google-genai.")
        if not self.api_keys:
            raise ValueError("API keys list cannot be empty for RotatingGeminiClient.")
        logger.info(f"RotatingGeminiClient initialized with {len(self.api_keys)} keys for model {self.model_name}.")

    def _get_next_client(self) -> Tuple[str, ChatGoogleGenerativeAI]:
        """Get the next client in rotation (synchronous version)"""
        if not self.api_keys:
            raise ValueError("API keys list is empty.")
        
        with self._lock:
            key_index = self._current_key_index
            key_to_use = self.api_keys[key_index]
            self._current_key_index = (self._current_key_index + 1) % len(self.api_keys)
            
            if key_to_use not in self._client_instances:
                logger.info(f"Creating new Gemini client for key index {key_index} (key starting with {key_to_use[:5]}...).")
                try:
                    self._client_instances[key_to_use] = ChatGoogleGenerativeAI(
                        model=self.model_name,
                        google_api_key=key_to_use,
                        temperature=self.temperature,
                        top_p=self.top_p,
                        top_k=self.top_k,
                        convert_system_message_to_human=self.convert_system_message_to_human,
                        request_timeout=self.request_timeout,
                        **self.other_model_kwargs,
                    )
                except Exception as e:
                    logger.error(f"Failed to initialize ChatGoogleGenerativeAI with key index {key_index}: {e}", exc_info=True)
                    raise
            
        return key_to_use, self._client_instances[key_to_use]

    async def _get_next_client_async(self) -> ChatGoogleGenerativeAI:
        """Get the next client in rotation (async version)"""
        # Use a sync method with lock first to avoid race conditions
        _, client = self._get_next_client()
        return client

    @property
    def _llm_type(self) -> str:
        return "rotating-gemini-chat"
    
    async def verify_client(self) -> bool:
        """
        Perform a verification call to ensure API keys are working.
        Returns True if successful, False otherwise.
        """
        try:
            test_message = [HumanMessage(content="Test connection. Respond with OK only.")]
            client = await self._get_next_client_async()
            result = await client.agenerate(messages=[test_message], stop=None)
            self._verified_api_keys = True
            logger.info(f"Verified Gemini client connection with test call.")
            return True
        except Exception as e:
            logger.error(f"Failed verification call to Gemini API: {e}", exc_info=True)
            return False
    
    # CRITICAL METHOD: Properly structured output implementation that Browser Agent calls
    def with_structured_output(
        self,
        schema: Any,
        *,
        include_raw: bool = False,
        method: str = "function_calling",
        **kwargs: Any,
    ) -> BaseChatModel:
        """Create a structured output version of this chat model."""
        # Create a new instance of our client to handle the structured output
        structured_instance = self.__class__(
            api_keys=self.api_keys,
            model_name=self.model_name,
            temperature=self.temperature,
            top_p=self.top_p,
            top_k=self.top_k,
            convert_system_message_to_human=self.convert_system_message_to_human,
            request_timeout=self.request_timeout,
            **self.other_model_kwargs
        )
        
        # Save schema information for use in _call_structured
        structured_instance._schema = schema
        structured_instance._include_raw = include_raw
        structured_instance._method = method
        structured_instance._kwargs = kwargs
        
        # Override ainvoke method to use structured output functionality
        async def _ainvoke_structured(
            self_instance,
            messages: List[BaseMessage],
            **kwargs_inner: Any
        ) -> Any:
            try:
                # Get a fresh client for this request
                client = await self_instance._get_next_client_async()
                
                # Create a structured output wrapper with this client
                structured_client = client.with_structured_output(
                    schema=self_instance._schema,
                    include_raw=self_instance._include_raw,
                    method=self_instance._method,
                    **self_instance._kwargs
                )
                
                # Call with proper argument conversion - IMPORTANT
                # This fixes the 'list' object has no attribute 'items' error
                # by ensuring arguments are passed correctly
                try:
                    return await structured_client.ainvoke(messages, **kwargs_inner)
                except Exception as e:
                    logger.error(f"Error in structured output client call: {e}", exc_info=True)
                    raise RuntimeError(f"Structured output call failed: {e}")
            except Exception as e:
                logger.error(f"Error in _ainvoke_structured: {e}", exc_info=True)
                raise
        
        # Monkey patch the ainvoke method on our instance
        structured_instance.ainvoke = _ainvoke_structured.__get__(structured_instance)
        
        # Return the enhanced instance
        return structured_instance
    
    async def ainvoke(
        self, 
        messages: List[BaseMessage], 
        **kwargs: Any
    ) -> Any:
        """Base ainvoke implementation that delegates to agenerate"""
        result = await self._agenerate(messages=messages, **kwargs)
        return result.generations[0][0].message
        
    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Async generation with proper callback handling"""
        client = await self._get_next_client_async()
        active_key_index_for_log = (self._current_key_index - 1 + len(self.api_keys)) % len(self.api_keys)
        
        try:
            # Properly format messages for agenerate
            # ChatGoogleGenerativeAI.agenerate expects List[List[BaseMessage]]
            return await client.agenerate(
                messages=[messages], 
                stop=stop,
                callbacks=run_manager,  # Correctly pass callbacks
                **kwargs
            )
        except ResourceExhausted as e:
            logger.warning(f"API Key (index {active_key_index_for_log}) exhausted: {e}")
            raise ConnectionError(f"ResourceExhausted with current key: {e}") from e
        except (ClientError, GoogleAPICallError) as e:
            logger.error(f"Google API call error with key (index {active_key_index_for_log}): {e}", exc_info=True)
            raise ConnectionAbortedError(f"Google API Call Error: {e}") from e
        except Exception as e:
            logger.error(f"Unexpected error during _agenerate with key (index {active_key_index_for_log}): {e}", exc_info=True)
            raise RuntimeError(f"LLM generation failed: {e}") from e

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """
        Sync generation method that safely handles the async/sync boundary 
        for ARQ worker environment
        """
        logger.debug("_generate (sync) called on RotatingGeminiClient.")

        _, client = self._get_next_client()
        active_key_index_for_log = (self._current_key_index - 1 + len(self.api_keys)) % len(self.api_keys)
        
        try:
            # If we're in a running event loop, this will cause issues
            current_loop = None
            try:
                current_loop = asyncio.get_event_loop()
            except RuntimeError:
                # No event loop in thread
                pass
                
            if current_loop and current_loop.is_running():
                try:
                    # Try to create a nested task in the existing loop
                    future = asyncio.run_coroutine_threadsafe(
                        self._agenerate(messages=messages, stop=stop, run_manager=None, **kwargs),
                        current_loop
                    )
                    return future.result(timeout=self.request_timeout or 120.0)
                except Exception as e:
                    error_msg = f"Error in _generate with running event loop: {str(e)}"
                    logger.error(error_msg)
                    return ChatResult(
                        generations=[
                            ChatGeneration(
                                message=AIMessage(content=f"Error in sync _generate: {error_msg}"),
                            )
                        ],
                        llm_output={"error": error_msg}
                    )
            
            # If we don't have a running loop, we can use asyncio.run
            try:
                return asyncio.run(self._agenerate(
                    messages=messages, 
                    stop=stop, 
                    run_manager=None,  # Can't pass sync run_manager to async context
                    **kwargs
                ))
            except (RuntimeError, asyncio.InvalidStateError) as e:
                # Handle case where we're in a partially initialized event loop
                logger.warning(f"Event loop issue in _generate: {e}. Creating new loop.")
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    result = loop.run_until_complete(self._agenerate(
                        messages=messages, stop=stop, run_manager=None,
                        **kwargs
                    ))
                    return result
                finally:
                    loop.close()
                    
        except ResourceExhausted as e:
            logger.warning(f"API Key (index {active_key_index_for_log}) exhausted: {e}")
            raise ConnectionError(f"ResourceExhausted with current key: {e}") from e
        except (ClientError, GoogleAPICallError) as e:
            logger.error(f"Google API call error with key (index {active_key_index_for_log}): {e}", exc_info=True)
            raise ConnectionAbortedError(f"Google API Call Error: {e}") from e
        except Exception as e:
            logger.error(f"Unexpected error during _generate with key (index {active_key_index_for_log}): {e}", exc_info=True)
            raise RuntimeError(f"LLM generation failed: {e}") from e

    async def _astream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> AsyncIterator[Any]:
        """Async streaming with proper callback handling"""
        client = await self._get_next_client_async()
        active_key_index_for_log = (self._current_key_index - 1 + len(self.api_keys)) % len(self.api_keys)
        
        try:
            async for chunk in client.astream(
                messages=messages,
                stop=stop,
                callbacks=run_manager,  # Correctly pass callbacks
                **kwargs
            ):
                yield chunk
        except ResourceExhausted as e:
            logger.warning(f"API Key (index {active_key_index_for_log}) exhausted during stream: {e}")
            raise ConnectionError(f"ResourceExhausted with current key during stream: {e}") from e
        except (ClientError, GoogleAPICallError) as e:
            logger.error(f"Google API call error during stream (key index {active_key_index_for_log}): {e}", exc_info=True)
            raise ConnectionAbortedError(f"Google API Call Error during stream: {e}") from e
        except Exception as e:
            logger.error(f"Unexpected error during _astream (key index {active_key_index_for_log}): {e}", exc_info=True)
            raise RuntimeError(f"LLM streaming failed: {e}") from e

    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[Any]:
        """
        Sync streaming method that safely handles the async/sync boundary
        for ARQ worker environment
        """
        logger.debug("_stream (sync) called on RotatingGeminiClient.")
        
        _, client = self._get_next_client()
        
        # Safe method that returns an empty iterator or error if issues
        try:
            current_loop = asyncio.get_event_loop()
            if current_loop.is_running():
                logger.error("Cannot run sync _stream from within a running event loop.")
                # Return empty iterator - can't yield from a running loop
                return iter([])
            
            # If no loop is running, we can create one
            return client._stream(
                messages=messages,
                stop=stop,
                run_manager=run_manager,
                **kwargs
            )
        except Exception as e:
            logger.error(f"Error in _stream: {e}", exc_info=True)
            return iter([])  # Return empty iterator on error
# --- END OF FIXED llm_service.py ---